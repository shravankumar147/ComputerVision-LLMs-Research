{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3004,"databundleVersionId":861823,"sourceType":"competition"}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/dinov2-cluster-analysis?scriptVersionId=184531756\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"To explore the power of feature representations of the DINO v2 model using the digit recognizer dataset from Kaggle, we'll follow a systematic approach. Here's a step-by-step plan to achieve this:\n\n1. **Load the Data**: Load the train and test datasets from the specified paths.\n2. **Preprocess the Data**: Prepare the images by reshaping them to the format required by the DINO v2 model.\n3. **Compute Embeddings**: Use the DINO v2 model to compute embeddings for the images.\n4. **Dimensionality Reduction**: Reduce the dimensionality of these embeddings to 2D space using PCA.\n5. **Clustering**: Apply a clustering algorithm (e.g., K-Means) to group the embeddings into 10 clusters.\n6. **Visualization**: Visualize the 2D embeddings, possibly overlaying the digit images, to observe the clustering.\n\n### Step-by-Step Implementation\n\n#### 1. Load the Data\n\nWe'll start by loading the data from the given paths.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the train and test datasets\ntrain_path = '/kaggle/input/digit-recognizer/train.csv'\ntest_path = '/kaggle/input/digit-recognizer/test.csv'\n\ntrain_data = pd.read_csv(train_path)\ntest_data = pd.read_csv(test_path)\n\n# Inspect the data\ntrain_data.head()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 2. Preprocess the Data\n\nThe images are provided as flattened arrays. We need to reshape them into 28x28 images and normalize them.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\n# Separate labels and images\nX_train = train_data.drop(columns=['label']).values\ny_train = train_data['label'].values\n\n# Reshape the images to 28x28 and normalize to [0, 1]\nX_train = X_train.reshape(-1, 28, 28, 1) / 255.0\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 3. Compute Embeddings\n\nFor this step, we'll use the DINO v2 model. This model expects images to be in a specific format, so we may need to preprocess our images further.","metadata":{}},{"cell_type":"code","source":"# from transformers import DINOModel, DINOFeatureExtractor\nfrom transformers import AutoImageProcessor, Dinov2Model\n\nimport torch\n\n# Initialize DINO v2 model and preprocessor\nimage_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\nmodel = Dinov2Model.from_pretrained(\"facebook/dinov2-base\")\n\n# basic usage of the model\n# inputs = image_processor(image, return_tensors=\"pt\")\n\n# with torch.no_grad():\n#     outputs = model(**inputs)\n\n# last_hidden_states = outputs.last_hidden_state\n# list(last_hidden_states.shape)\n\n\n# Preprocess the images to match DINO input requirements\n# DINO expects 3-channel images, so we need to convert grayscale to RGB by repeating channels\nX_train_rgb = np.repeat(X_train, 3, axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_rgb.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute embeddings\nembeddings = []\nwith torch.no_grad():\n    for image in X_train_rgb:\n        # Prepare the images for DINO\n        inputs = image_processor(images=image, return_tensors=\"pt\")\n        embedding = model(**inputs).last_hidden_state[:, 0, :]\n        embeddings.append(embedding.detach().cpu())\nembeddings = torch.cat(embeddings, dim=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings.shape","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Ensure that we are using GPU if available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Convert the image array to a PyTorch tensor and move to the GPU\nX_train_rgb_tensor = torch.tensor(X_train_rgb, dtype=torch.float32).to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a DataLoader for batch processing\nbatch_size = 64  # You can adjust the batch size depending on your GPU memory\ndataset = TensorDataset(X_train_rgb_tensor)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Move the model to GPU if available\nmodel.to(device)\n\n# Compute embeddings using batch processing\nembeddings = []\nwith torch.no_grad():\n    for batch in dataloader:\n        batch_images = batch[0].to(device)  # Get the batch of images and move to the GPU\n        inputs = image_processor(images=batch_images, return_tensors=\"pt\", do_resize=False, do_normalize=False).to(device)\n        output = model(**inputs).last_hidden_state[:, 0, :]\n        embeddings.append(output.detach().cpu())  # Move to CPU and append to list\n\n# Concatenate the embeddings\nembeddings = torch.cat(embeddings, dim=0)\nprint(\"Embeddings shape:\", embeddings.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 4. Dimensionality Reduction\n\nWe'll use PCA to reduce the dimensionality of the embeddings to 2D.","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\n# Convert embeddings to numpy array for PCA\nembeddings_np = embeddings.numpy()\n\n# Apply PCA to reduce dimensions to 2D\npca = PCA(n_components=2)\nembeddings_2d = pca.fit_transform(embeddings_np)\n\n# Print the shape of the 2D embeddings\nprint(embeddings_2d.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 5. Clustering\n\nUsing K-Means to cluster the 2D embeddings into 10 clusters.","metadata":{}},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\n# Perform K-Means clustering with 10 clusters\nkmeans = KMeans(n_clusters=10, random_state=42)\nclusters = kmeans.fit_predict(embeddings_2d)\n\n# Print the unique clusters\nprint(np.unique(clusters))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 6. Visualization\n\nFinally, visualize the 2D embeddings and the clusters.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Scatter plot of 2D embeddings colored by clusters\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='tab10', alpha=0.6)\nplt.colorbar(scatter, ticks=range(10))\nplt.title('2D Visualization of DINO v2 Embeddings with K-Means Clustering')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Fancy Plot with Images\n\nFor a more visually appealing plot, overlay the actual digit images on the 2D plot. This helps in verifying if similar digits are grouped together.","metadata":{}},{"cell_type":"code","source":"from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n\ndef imscatter(x, y, images, ax=None, zoom=0.1):\n    if ax is None:\n        ax = plt.gca()\n    for i, (x0, y0) in enumerate(zip(x, y)):\n        image = images[i].reshape(28, 28)\n        im = OffsetImage(image, zoom=zoom, cmap='gray')\n        ab = AnnotationBbox(im, (x0, y0), frameon=False)\n        ax.add_artist(ab)\n    ax.update_datalim(np.column_stack([x, y]))\n    ax.autoscale()\n\nplt.figure(figsize=(15, 12))\nimscatter(embeddings_2d[:, 0], embeddings_2d[:, 1], X_train.squeeze(), zoom=0.6)\nplt.colorbar(scatter, ticks=range(10))\nplt.title('2D Visualization of DINO v2 Embeddings with Overlaid Images')\nplt.xlabel('PCA Component 1')\nplt.ylabel('PCA Component 2')\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Conclusion\n\nFollowing this approach, you will be able to visually assess the clustering of digit images based on the feature representations provided by the DINO v2 model. This helps in understanding the model's capability to compress similar images into similar regions of the embedding space.\n\nWould you like to proceed with any specific part of the implementation?","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}