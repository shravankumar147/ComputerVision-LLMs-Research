{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/ibm-granite-hugging-face-transformers?scriptVersionId=186907952\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Granite, developed by IBM Research\nGranite is IBM's flagship series of LLM foundation models based on decoder-only transformer architecture. Granite language models are trained on trusted enterprise data spanning internet, academic, code, legal and finance. Currently we have four models in the Granite series.\n\n* Granite 13b chat: Chat model optimized for dialogue use cases and works well with virtual agent and chat applications\n* Granite 13b instruct: Instruct model trained on high-quality finance data to perform well in finance domain tasks\n* Granite multilingual: Trained to understand and generate text in English, German, Spanish, French and Portuguese\n* Granite Japanese: Designed to perform language tasks on Japanese text","metadata":{}},{"cell_type":"markdown","source":"## Import AutoModel and Tokenizer Classes from huggingface transformers\n\nReference: https://huggingface.co/ibm-granite","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:03:03.017762Z","iopub.execute_input":"2024-07-05T04:03:03.018283Z","iopub.status.idle":"2024-07-05T04:03:03.023239Z","shell.execute_reply.started":"2024-07-05T04:03:03.018253Z","shell.execute_reply":"2024-07-05T04:03:03.022143Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Download model and tokenizer","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" # or \"cpu\"\nmodel_path = \"ibm-granite/granite-3b-code-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\n# drop device_map if running on CPU\nmodel = AutoModelForCausalLM.from_pretrained(model_path, device_map=device)\nmodel.eval()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:03:04.338974Z","iopub.execute_input":"2024-07-05T04:03:04.339709Z","iopub.status.idle":"2024-07-05T04:03:50.170193Z","shell.execute_reply.started":"2024-07-05T04:03:04.339677Z","shell.execute_reply":"2024-07-05T04:03:50.169285Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/4.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf56d51609fb47afa7d96235377fae0f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c75458a498c4bdf8cbd4b360a3d1a31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51c14c2e3a7a49729a2a1589a3deb3df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/680 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1928b9626c1475b92e899053a932cc0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/41.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f6285005714a8cad0557e28420cef1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08581dfd68964311ab1055d022ed5e0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"770f9ddc7f0a42259cfed3ac94f291e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff791f2627b24106baacf12a22dfcbae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac12a3e387e44fd1a4c113ab26863f0e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21d17c820bd84ccda7f3c8bbe26d6cd7"}},"metadata":{}},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(49152, 2560, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (k_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (v_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (o_proj): Linear(in_features=2560, out_features=2560, bias=True)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear(in_features=2560, out_features=10240, bias=True)\n          (up_proj): Linear(in_features=2560, out_features=10240, bias=True)\n          (down_proj): Linear(in_features=10240, out_features=2560, bias=True)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm()\n        (post_attention_layernorm): LlamaRMSNorm()\n      )\n    )\n    (norm): LlamaRMSNorm()\n  )\n  (lm_head): Linear(in_features=2560, out_features=49152, bias=False)\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"## Let's set up input - a starting prompt","metadata":{}},{"cell_type":"code","source":"# change input text as desired\ninput_text = \"def generate():\"\n# tokenize the text\ninput_tokens = tokenizer(input_text, return_tensors=\"pt\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:07:19.70466Z","iopub.execute_input":"2024-07-05T04:07:19.705544Z","iopub.status.idle":"2024-07-05T04:07:19.713447Z","shell.execute_reply.started":"2024-07-05T04:07:19.705508Z","shell.execute_reply":"2024-07-05T04:07:19.712653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"input_tokens","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:07:23.884097Z","iopub.execute_input":"2024-07-05T04:07:23.8845Z","iopub.status.idle":"2024-07-05T04:07:23.892985Z","shell.execute_reply.started":"2024-07-05T04:07:23.884473Z","shell.execute_reply":"2024-07-05T04:07:23.892194Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[ 589, 4450, 2262]]), 'attention_mask': tensor([[1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"# transfer tokenized inputs to the device\nfor i in input_tokens:\n    input_tokens[i] = input_tokens[i].to(device)\n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:08:00.108657Z","iopub.execute_input":"2024-07-05T04:08:00.109308Z","iopub.status.idle":"2024-07-05T04:08:00.114078Z","shell.execute_reply.started":"2024-07-05T04:08:00.109275Z","shell.execute_reply":"2024-07-05T04:08:00.113017Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Let's generate the text using model over our input","metadata":{}},{"cell_type":"code","source":"# generate output tokens\noutput = model.generate(**input_tokens, max_new_tokens=200)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:11:33.722188Z","iopub.execute_input":"2024-07-05T04:11:33.722546Z","iopub.status.idle":"2024-07-05T04:11:41.557934Z","shell.execute_reply.started":"2024-07-05T04:11:33.722522Z","shell.execute_reply":"2024-07-05T04:11:41.557159Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"output.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:11:41.559847Z","iopub.execute_input":"2024-07-05T04:11:41.560316Z","iopub.status.idle":"2024-07-05T04:11:41.566837Z","shell.execute_reply.started":"2024-07-05T04:11:41.560281Z","shell.execute_reply":"2024-07-05T04:11:41.565686Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 203])"},"metadata":{}}]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:11:41.567979Z","iopub.execute_input":"2024-07-05T04:11:41.56832Z","iopub.status.idle":"2024-07-05T04:11:41.585971Z","shell.execute_reply.started":"2024-07-05T04:11:41.56829Z","shell.execute_reply":"2024-07-05T04:11:41.58515Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"tensor([[  589,  4450,  2262,   284,  1524,   284, 10945,   312,   537,   739,\n           432,  3995,  7515,    32,   284,  1524,   284,  3649,  3995,    81,\n         14951,   284,  3995,    81, 14951,   280,   428,  3855,    32, 21378,\n            26,    34,    30,   225,    35,    34,    34,    27,   436,   617,\n           328,  2155,    26,    35,    34,  2177,   284,  1459,    26,  3855,\n            81, 14951,    27,   203,   203,   589,   622,    81,  3855,    81,\n          2171,    26,  1055,   711,   284,  1524,   284,  1390,   312,  3995,\n          1451,   645,   322,   739,    32,   284,  1524,   284,   442,  3995,\n            81, 14951,    77,  1055,    79,   203,   203,   589,  2575,  2262,\n           284,  1524,   284,  6688,   667,   372,  1420,   322,  3460,    32,\n           284,  1524,   284,  4450,   346,   284,  1459,    26,   371,    81,\n          3855,    81,  2171,    26,    39,   490,   203,   203,   325,  1156,\n           426,   505,   610,  9602,  1831, 18018,   284,  2575,   346,   203,\n           914,   203,   203,   383,   458,  1340,    30,   996,  1932,  1188,\n           322,   529,  3855,    82,  2297,   372,  2857,   322,  3995,  1451,\n         11745,  4442,    32,  2688,  2329,  5597,   312,  3649,  2677,   529,\n          3855,    81, 14951,    82,   372,  2968,   322,  4723,  3995,  7515,\n            32,   203,   203,  1318,   529,  5536,  6620,   667,   438, 18392,\n           436, 17335,   312,   537,   739,   432,  3995,  7515,    32,  2030,\n          5251,   312,  1149, 32231,  1453,   372,  1487,   312,  1149,   432,\n           225,    35,    34]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Let's decode and see the text","metadata":{}},{"cell_type":"code","source":"# decode output tokens into text\noutput = tokenizer.batch_decode(output)\n# loop over the batch to print, in this example the batch size is 1\nfor i in output:\n    print(i)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-05T04:11:43.601455Z","iopub.execute_input":"2024-07-05T04:11:43.60185Z","iopub.status.idle":"2024-07-05T04:11:43.608459Z","shell.execute_reply.started":"2024-07-05T04:11:43.601822Z","shell.execute_reply":"2024-07-05T04:11:43.607087Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"def generate():\n    \"\"\"\n    Generate a new set of random numbers.\n    \"\"\"\n    global random_numbers\n    random_numbers = [random.randint(0, 100) for _ in range(10)]\n    print(random_numbers)\n\ndef get_random_number(index):\n    \"\"\"\n    Get a random number from the set.\n    \"\"\"\n    return random_numbers[index]\n\ndef main():\n    \"\"\"\n    Main function to run the program.\n    \"\"\"\n    generate()\n    print(get_random_number(5))\n\nif __name__ == \"__main__\":\n    main()\n```\n\nIn this code, we first import the `random` module to access the random number generation functions. We also define a global variable `random_numbers` to store the generated random numbers.\n\nThe `generate()` function is responsible for generating a new set of random numbers. It uses a list comprehension to create a list of 10\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Try out the generated code ","metadata":{}},{"cell_type":"code","source":"import random\ndef generate():\n    \"\"\"\n    Generate a new set of random numbers.\n    \"\"\"\n    global random_numbers\n    random_numbers = [random.randint(0, 100) for _ in range(10)]\n    print(random_numbers)\n\ndef get_random_number(index):\n    \"\"\"\n    Get a random number from the set.\n    \"\"\"\n    return random_numbers[index]\n\ndef main():\n    \"\"\"\n    Main function to run the program.\n    \"\"\"\n    generate()\n    print(get_random_number(5))\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.status.busy":"2024-07-05T04:12:37.512971Z","iopub.execute_input":"2024-07-05T04:12:37.513354Z","iopub.status.idle":"2024-07-05T04:12:37.520801Z","shell.execute_reply.started":"2024-07-05T04:12:37.513323Z","shell.execute_reply":"2024-07-05T04:12:37.519784Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"[24, 85, 36, 86, 58, 55, 47, 93, 54, 69]\n55\n","output_type":"stream"}]}]}