{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/document-loaders-for-rags?scriptVersionId=209670575\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Document Loaders for RAG","metadata":{}},{"cell_type":"code","source":"!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl pacmap datasets langchain-community ragatouille","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:17:34.01736Z","iopub.execute_input":"2024-11-26T06:17:34.018354Z","iopub.status.idle":"2024-11-26T06:18:14.867423Z","shell.execute_reply.started":"2024-11-26T06:17:34.018269Z","shell.execute_reply":"2024-11-26T06:18:14.866115Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.2.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\nthinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nydata-profiling 4.10.0 requires scipy<1.14,>=1.4.1, but you have scipy 1.14.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/document_loaders/pdf/#using-pypdf","metadata":{}},{"cell_type":"code","source":"# https://arxiv.org/pdf/1706.03762","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download PDF Helper Function","metadata":{}},{"cell_type":"code","source":"import requests\n\ndef download_pdf(url, save_path):\n    \"\"\"\n    Downloads a PDF from the given URL and saves it to the specified path.\n\n    Parameters:\n        url (str): The URL of the PDF to download.\n        save_path (str): The file path where the PDF should be saved.\n\n    Returns:\n        bool: True if the download was successful, False otherwise.\n    \"\"\"\n    try:\n        # Send a GET request to download the file\n        response = requests.get(url, stream=True)\n        response.raise_for_status()  # Raise an exception for HTTP errors\n\n        # Write the content to a file\n        with open(save_path, \"wb\") as file:\n            for chunk in response.iter_content(chunk_size=8192):\n                file.write(chunk)\n\n        print(f\"PDF downloaded and saved as '{save_path}'\")\n        return True\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to download PDF. Error: {e}\")\n        return False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:24:34.126786Z","iopub.execute_input":"2024-11-26T06:24:34.12741Z","iopub.status.idle":"2024-11-26T06:24:34.223035Z","shell.execute_reply.started":"2024-11-26T06:24:34.127354Z","shell.execute_reply":"2024-11-26T06:24:34.221964Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"## Downloading a single pdf file from given URL","metadata":{}},{"cell_type":"code","source":"# Example usage\nurl = \"https://arxiv.org/pdf/1706.03762\"\nsave_path = \"Attention_Is_All_You_Need.pdf\"\ndownload_pdf(url, save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:24:42.914224Z","iopub.execute_input":"2024-11-26T06:24:42.915115Z","iopub.status.idle":"2024-11-26T06:24:43.356197Z","shell.execute_reply.started":"2024-11-26T06:24:42.915063Z","shell.execute_reply":"2024-11-26T06:24:43.355164Z"}},"outputs":[{"name":"stdout","text":"PDF downloaded and saved as 'Attention_Is_All_You_Need.pdf'\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## Download Multiple PDFs from a given list of URLs","metadata":{}},{"cell_type":"code","source":"pdf_urls = [\n    \"https://arxiv.org/pdf/1706.03762\",\n    \"https://arxiv.org/pdf/1801.06146\",\n    \"https://arxiv.org/pdf/2103.15348\",\n]\n\nfor i, url in enumerate(pdf_urls, start=1):\n    save_path = f\"paper_{i}.pdf\"\n    download_pdf(url, save_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:37:23.915045Z","iopub.execute_input":"2024-11-26T06:37:23.915466Z","iopub.status.idle":"2024-11-26T06:37:26.334694Z","shell.execute_reply.started":"2024-11-26T06:37:23.91543Z","shell.execute_reply":"2024-11-26T06:37:26.333522Z"}},"outputs":[{"name":"stdout","text":"PDF downloaded and saved as 'paper_1.pdf'\nPDF downloaded and saved as 'paper_2.pdf'\nPDF downloaded and saved as 'paper_3.pdf'\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Exploring the langchain document loaders","metadata":{}},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\n\nloader = PyPDFLoader(\"paper_1.pdf\")\npages = loader.load_and_split()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:26:33.524943Z","iopub.execute_input":"2024-11-26T06:26:33.525363Z","iopub.status.idle":"2024-11-26T06:26:36.029291Z","shell.execute_reply.started":"2024-11-26T06:26:33.525327Z","shell.execute_reply":"2024-11-26T06:26:36.028074Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"pages[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:26:59.92304Z","iopub.execute_input":"2024-11-26T06:26:59.923585Z","iopub.status.idle":"2024-11-26T06:26:59.930508Z","shell.execute_reply.started":"2024-11-26T06:26:59.923548Z","shell.execute_reply":"2024-11-26T06:26:59.929269Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Document(metadata={'source': 'paper_1.pdf', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"print(pages[0].page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:28:11.163971Z","iopub.execute_input":"2024-11-26T06:28:11.164361Z","iopub.status.idle":"2024-11-26T06:28:11.170311Z","shell.execute_reply.started":"2024-11-26T06:28:11.164329Z","shell.execute_reply":"2024-11-26T06:28:11.169095Z"}},"outputs":[{"name":"stdout","text":"Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani∗\nGoogle Brain\navaswani@google.com\nNoam Shazeer∗\nGoogle Brain\nnoam@google.com\nNiki Parmar∗\nGoogle Research\nnikip@google.com\nJakob Uszkoreit∗\nGoogle Research\nusz@google.com\nLlion Jones∗\nGoogle Research\nllion@google.com\nAidan N. Gomez∗ †\nUniversity of Toronto\naidan@cs.toronto.edu\nŁukasz Kaiser∗\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin∗ ‡\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n†Work performed while at Google Brain.\n‡Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Extracting Image Content from PDFs (OCR compatibility)","metadata":{}},{"cell_type":"markdown","source":"In order to extract image content as text from pdf, we need to pass  extract_images=True in PyPDFLoader\n`PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)`\nThis requires `rapidocr-onnxruntime` ","metadata":{}},{"cell_type":"code","source":"!pip install rapidocr-onnxruntime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:38:09.021161Z","iopub.execute_input":"2024-11-26T06:38:09.021569Z","iopub.status.idle":"2024-11-26T06:38:21.682952Z","shell.execute_reply.started":"2024-11-26T06:38:09.021532Z","shell.execute_reply":"2024-11-26T06:38:21.681491Z"}},"outputs":[{"name":"stdout","text":"Collecting rapidocr-onnxruntime\n  Downloading rapidocr_onnxruntime-1.4.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: pyclipper>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.3.0.post5)\nRequirement already satisfied: opencv-python>=4.5.1.48 in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (4.10.0.84)\nRequirement already satisfied: numpy<3.0.0,>=1.19.5 in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.26.4)\nRequirement already satisfied: six>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.16.0)\nRequirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.8.5.post1)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (6.0.2)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (10.3.0)\nCollecting onnxruntime>=1.7.0 (from rapidocr-onnxruntime)\n  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from rapidocr-onnxruntime) (4.66.4)\nCollecting coloredlogs (from onnxruntime>=1.7.0->rapidocr-onnxruntime)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.25)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.2)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.12)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\nDownloading rapidocr_onnxruntime-1.4.0-py3-none-any.whl (14.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, rapidocr-onnxruntime\nSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.20.1 rapidocr-onnxruntime-1.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"loader = PyPDFLoader(\"https://arxiv.org/pdf/2103.15348.pdf\", extract_images=True)\npages = loader.load()\npages[4].page_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:40:01.083406Z","iopub.execute_input":"2024-11-26T06:40:01.083868Z","iopub.status.idle":"2024-11-26T06:40:39.756556Z","shell.execute_reply.started":"2024-11-26T06:40:01.083829Z","shell.execute_reply":"2024-11-26T06:40:39.755545Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'LayoutParser: A Uniﬁed Toolkit for DL-Based DIA 5\\nTable 1: Current layout detection models in the LayoutParser model zoo\\nDataset Base Model1 Large ModelNotes\\nPubLayNet [38] F / M M Layouts of modern scientiﬁc documents\\nPRImA [3] M - Layouts of scanned modern magazines and scientiﬁc reports\\nNewspaper [17] F - Layouts of scanned US newspapers from the 20th century\\nTableBank [18] F F Table region on modern scientiﬁc and business document\\nHJDataset [31] F / M - Layouts of history Japanese documents\\n1For each dataset, we train several models of diﬀerent sizes for diﬀerent needs (the trade-oﬀ between accuracy\\nvs. computational cost). For “base model” and “large model”, we refer to using the ResNet 50 or ResNet 101\\nbackbones [13], respectively. One can train models of diﬀerent architectures, like Faster R-CNN [28] (F) and Mask\\nR-CNN [12] (M). For example, an F in the Large Model column indicates it has a Faster R-CNN model trained\\nusing the ResNet 101 backbone. The platform is maintained and a number of additions will be made to the model\\nzoo in coming months.\\nlayout data structures, which are optimized for eﬃciency and versatility. 3) When\\nnecessary, users can employ existing or customized OCR models via the uniﬁed\\nAPI provided in the OCR module. 4) LayoutParser comes with a set of utility\\nfunctions for the visualization and storage of the layout data. 5) LayoutParser\\nis also highly customizable, via its integration with functions for layout data\\nannotation and model training . We now provide detailed descriptions for each\\ncomponent.\\n3.1 Layout Detection Models\\nIn LayoutParser, a layout model takes a document image as an input and\\ngenerates a list of rectangular boxes for the target content regions. Diﬀerent\\nfrom traditional methods, it relies on deep convolutional neural networks rather\\nthan manually curated rules to identify content regions. It is formulated as an\\nobject detection problem and state-of-the-art models like Faster R-CNN [ 28] and\\nMask R-CNN [12] are used. This yields prediction results of high accuracy and\\nmakes it possible to build a concise, generalized interface for layout detection.\\nLayoutParser, built upon Detectron2 [ 35], provides a minimal API that can\\nperform layout detection with only four lines of code in Python:\\n1 import layoutparser as lp\\n2 image = cv2 . imread (\" image_file \") # load images\\n3 model = lp. Detectron2LayoutModel (\\n4 \"lp :// PubLayNet / faster_rcnn_R_50_FPN_3x / config \")\\n5 layout = model . detect ( image )\\nLayoutParser provides a wealth of pre-trained model weights using various\\ndatasets covering diﬀerent languages, time periods, and document types. Due to\\ndomain shift [7], the prediction performance can notably drop when models are ap-\\nplied to target samples that are signiﬁcantly diﬀerent from the training dataset. As\\ndocument structures and layouts vary greatly in diﬀerent domains, it is important\\nto select models trained on a dataset similar to the test samples. A semantic syntax\\nis used for initializing the model weights in LayoutParser, using both the dataset\\nname and model name lp://<dataset-name>/<model-architecture-name>.'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"print(pages[3].page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-26T06:42:21.123425Z","iopub.execute_input":"2024-11-26T06:42:21.123847Z","iopub.status.idle":"2024-11-26T06:42:21.129467Z","shell.execute_reply.started":"2024-11-26T06:42:21.12381Z","shell.execute_reply":"2024-11-26T06:42:21.128395Z"}},"outputs":[{"name":"stdout","text":"4 Z. Shen et al.\nEfficient Data Annotation\nC u s t o m i z e d  M o d e l  T r a i n i n g\nModel Cust omization\nDI A Model Hub\nDI A Pipeline Sharing\nCommunity Platform\nLa y out Detection Models\nDocument Images \nT h e  C o r e  L a y o u t P a r s e r  L i b r a r y\nOCR Module St or age & VisualizationLa y out Data Structur e\nFig. 1: The overall architecture of LayoutParser. For an input document image,\nthe core LayoutParser library provides a set of oﬀ-the-shelf tools for layout\ndetection, OCR, visualization, and storage, backed by a carefully designed layout\ndata structure. LayoutParser also supports high level customization via eﬃcient\nlayout annotation and model training functions. These improve model accuracy\non the target samples. The community platform enables the easy sharing of DIA\nmodels and whole digitization pipelines to promote reusability and reproducibility.\nA collection of detailed documentation, tutorials and exemplar projects make\nLayoutParser easy to learn and use.\nAllenNLP [8] and transformers [ 34] have provided the community with complete\nDL-based support for developing and deploying models for general computer\nvision and natural language processing problems. LayoutParser, on the other\nhand, specializes speciﬁcally in DIA tasks. LayoutParser is also equipped with a\ncommunity platform inspired by established model hubs such as Torch Hub [23]\nand TensorFlow Hub [1]. It enables the sharing of pretrained models as well as\nfull document processing pipelines that are unique to DIA tasks.\nThere have been a variety of document data collections to facilitate the\ndevelopment of DL models. Some examples include PRImA [3](magazine layouts),\nPubLayNet [38](academic paper layouts), Table Bank [ 18](tables in academic\npapers), Newspaper Navigator Dataset [ 16, 17](newspaper ﬁgure layouts) and\nHJDataset [31](historical Japanese document layouts). A spectrum of models\ntrained on these datasets are currently available in the LayoutParser model zoo\nto support diﬀerent use cases.\n3 The Core LayoutParser Library\nAt the core of LayoutParser is an oﬀ-the-shelf toolkit that streamlines DL-\nbased document image analysis. Five components support a simple interface\nwith comprehensive functionalities: 1) The layout detection models enable using\npre-trained or self-trained DL models for layout detection with just four lines\nof code. 2) The detected layout information is stored in carefully engineered\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}