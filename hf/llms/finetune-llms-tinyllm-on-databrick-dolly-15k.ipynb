{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/finetune-llms-tinyllm-on-databrick-dolly-15k?scriptVersionId=206353195\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install required packages\n!pip install -U transformers datasets accelerate bitsandbytes peft trl","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:05:46.94294Z","iopub.execute_input":"2024-11-10T15:05:46.94337Z","iopub.status.idle":"2024-11-10T15:06:17.901325Z","shell.execute_reply.started":"2024-11-10T15:05:46.943333Z","shell.execute_reply":"2024-11-10T15:06:17.899881Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.1)\nCollecting transformers\n  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (3.0.1)\nCollecting datasets\n  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.34.2)\nCollecting accelerate\n  Downloading accelerate-1.1.1-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\nCollecting peft\n  Downloading peft-0.13.2-py3-none-any.whl.metadata (13 kB)\nCollecting trl\n  Downloading trl-0.12.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.25.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.5.15)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.0)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.5)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.4.0)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from trl) (13.7.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.5)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->trl) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.46.2-py3-none-any.whl (10.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hDownloading datasets-3.1.0-py3-none-any.whl (480 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading accelerate-1.1.1-py3-none-any.whl (333 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.13.2-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading trl-0.12.0-py3-none-any.whl (310 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.2/310.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers, datasets, trl, peft\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.34.2\n    Uninstalling accelerate-0.34.2:\n      Successfully uninstalled accelerate-0.34.2\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.45.1\n    Uninstalling transformers-4.45.1:\n      Successfully uninstalled transformers-4.45.1\n  Attempting uninstall: datasets\n    Found existing installation: datasets 3.0.1\n    Uninstalling datasets-3.0.1:\n      Successfully uninstalled datasets-3.0.1\nSuccessfully installed accelerate-1.1.1 bitsandbytes-0.44.1 datasets-3.1.0 peft-0.13.2 transformers-4.46.2 trl-0.12.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Fine-Tuning LLMs","metadata":{}},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    TrainingArguments,\n    Trainer,\n    DataCollatorForLanguageModeling,\n    BitsAndBytesConfig\n)\nfrom peft import (\n    LoraConfig,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    TaskType\n)\nimport os\nfrom datetime import datetime","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:06:17.905343Z","iopub.execute_input":"2024-11-10T15:06:17.905853Z","iopub.status.idle":"2024-11-10T15:06:37.51999Z","shell.execute_reply.started":"2024-11-10T15:06:17.905788Z","shell.execute_reply":"2024-11-10T15:06:37.519158Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Check GPU availability\nprint(\"GPU Available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU Model:\", torch.cuda.get_device_name(0))\n    print(\"GPU Memory:\", torch.cuda.get_device_properties(0).total_memory / 1e9, \"GB\")\n\n# Configure quantization\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\"\n)\n\n# Load model with quantization\nmodel_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    device_map=\"auto\",\n    torch_dtype=torch.float16\n)\n\n# Prepare model for k-bit training\nmodel = prepare_model_for_kbit_training(model)\n\n# Configure LoRA\nlora_config = LoraConfig(\n    r=16,                     # Rank\n    lora_alpha=32,           # Alpha scaling\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"], # Target attention modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM\n)\n\n# Create PEFT model\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()  # Print trainable parameters info","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:06:43.305909Z","iopub.execute_input":"2024-11-10T15:06:43.306879Z","iopub.status.idle":"2024-11-10T15:07:40.72697Z","shell.execute_reply.started":"2024-11-10T15:06:43.306837Z","shell.execute_reply":"2024-11-10T15:07:40.725997Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU Available: True\nGPU Model: Tesla P100-PCIE-16GB\nGPU Memory: 17.059545088 GB\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddeced027b9042b3a4dac51124509498"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0edd4bbb7746d0bb89dbea3b003a85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c6d729642fa41859b14963ee38b99f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113e4e2ffb344c359dc39ae25f761403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4455953b281642ba8588ebc6c7af5bfe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b81a724321d4e5793df59a9969e5055"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"993f85becb0546e291cc15a30001f5dd"}},"metadata":{}},{"name":"stdout","text":"trainable params: 4,505,600 || all params: 1,104,553,984 || trainable%: 0.4079\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load a small subset of the dataset\ndataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train[:500]\")\n\ndef preprocess_function(examples):\n    \"\"\"Convert the dataset into a format suitable for training\"\"\"\n    texts = [\n        f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"\n        for instruction, response in zip(examples['instruction'], examples['response'])\n    ]\n    \n    tokenized = tokenizer(\n        texts,\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",\n        return_tensors=\"pt\"\n    )\n    \n    return tokenized\n\n# Preprocess the dataset\ntokenized_dataset = dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=dataset.column_names\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:07:47.57229Z","iopub.execute_input":"2024-11-10T15:07:47.573092Z","iopub.status.idle":"2024-11-10T15:07:50.040204Z","shell.execute_reply.started":"2024-11-10T15:07:47.573042Z","shell.execute_reply":"2024-11-10T15:07:50.039267Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c749d6aba824153a13f17768e11e2de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"databricks-dolly-15k.jsonl:   0%|          | 0.00/13.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26f2f4a1858049bf84e3a2d0bf88ae06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/15011 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56f6759a506a4f87b89332a59ee70d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd3a7b218fb4e7da0fa0c0567e1e313"}},"metadata":{}}]},{"cell_type":"code","source":"# Setup training arguments\ntraining_args = TrainingArguments(\n    output_dir=f\"./finetuned_tinyllama_lora_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n    num_train_epochs=3,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    save_steps=100,\n    save_total_limit=2,\n    learning_rate=2e-4,  # Slightly higher learning rate for LoRA\n    warmup_steps=50,\n    logging_dir='./logs',\n    logging_steps=10,\n    fp16=True,\n    optim=\"adamw_torch_fused\",\n    remove_unused_columns=True,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:07:57.518788Z","iopub.execute_input":"2024-11-10T15:07:57.519672Z","iopub.status.idle":"2024-11-10T15:07:57.547053Z","shell.execute_reply.started":"2024-11-10T15:07:57.51963Z","shell.execute_reply":"2024-11-10T15:07:57.546255Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Initialize data collator\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,\n    mlm=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:07:59.188449Z","iopub.execute_input":"2024-11-10T15:07:59.189305Z","iopub.status.idle":"2024-11-10T15:07:59.193667Z","shell.execute_reply.started":"2024-11-10T15:07:59.189263Z","shell.execute_reply":"2024-11-10T15:07:59.19255Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Initialize trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    data_collator=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:08:04.652278Z","iopub.execute_input":"2024-11-10T15:08:04.652676Z","iopub.status.idle":"2024-11-10T15:08:05.247189Z","shell.execute_reply.started":"2024-11-10T15:08:04.65264Z","shell.execute_reply":"2024-11-10T15:08:05.24635Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Clear CUDA cache before training\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:08:10.759841Z","iopub.execute_input":"2024-11-10T15:08:10.760244Z","iopub.status.idle":"2024-11-10T15:08:10.768421Z","shell.execute_reply.started":"2024-11-10T15:08:10.760205Z","shell.execute_reply":"2024-11-10T15:08:10.767318Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:08:12.49762Z","iopub.execute_input":"2024-11-10T15:08:12.498419Z","iopub.status.idle":"2024-11-10T15:15:54.690817Z","shell.execute_reply.started":"2024-11-10T15:08:12.49838Z","shell.execute_reply":"2024-11-10T15:15:54.689944Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='93' max='93' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [93/93 07:36, Epoch 2/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.275200</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.122300</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.862600</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.961400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.735100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.783700</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.800200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.683100</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.673400</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=93, training_loss=1.8702943812134445, metrics={'train_runtime': 461.821, 'train_samples_per_second': 3.248, 'train_steps_per_second': 0.201, 'total_flos': 2374746255654912.0, 'train_loss': 1.8702943812134445, 'epoch': 2.976})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the trained model and adapter\noutput_dir = training_args.output_dir\nmodel.save_pretrained(output_dir)\ntokenizer.save_pretrained(output_dir)\n\n# Print completion message\nprint(f\"\\nTraining completed! Model saved to {output_dir}\")\nprint(\"\\nTo use this model for inference, you'll need to:\")\nprint(\"1. Load the base model\")\nprint(\"2. Load the LoRA adapter weights\")\nprint(\"3. Merge them (optional) or use them together\")","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:19:10.276766Z","iopub.execute_input":"2024-11-10T15:19:10.277189Z","iopub.status.idle":"2024-11-10T15:19:10.587686Z","shell.execute_reply.started":"2024-11-10T15:19:10.277152Z","shell.execute_reply":"2024-11-10T15:19:10.586652Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"\nTraining completed! Model saved to ./finetuned_tinyllama_lora_20241110_1507\n\nTo use this model for inference, you'll need to:\n1. Load the base model\n2. Load the LoRA adapter weights\n3. Merge them (optional) or use them together\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Test the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport time\nimport json\nfrom datetime import datetime\n\nclass LoRAModelTester:\n    def __init__(self, base_model_name, adapter_path):\n        \"\"\"Initialize the model tester with base model and LoRA adapter\"\"\"\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"Using device: {self.device}\")\n        \n        print(\"Loading tokenizer...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        \n        print(\"Loading base model...\")\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        print(\"Loading LoRA adapter...\")\n        self.model = PeftModel.from_pretrained(\n            base_model,\n            adapter_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        # Optional: Merge weights for faster inference\n        print(\"Merging weights for optimized inference...\")\n        self.model = self.model.merge_and_unload()\n        \n        print(\"Model loading complete!\")\n        \n    def generate_response(self, instruction, max_length=256, temperature=0.7):\n        \"\"\"Generate a response for a given instruction\"\"\"\n        # Format the prompt\n        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n        \n        # Tokenize\n        inputs = self.tokenizer(prompt, return_tensors=\"pt\")\n        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n        \n        # Generate\n        start_time = time.time()\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_length=max_length,\n                temperature=temperature,\n                top_p=0.9,\n                do_sample=True,\n                pad_token_id=self.tokenizer.eos_token_id\n            )\n        end_time = time.time()\n        \n        # Decode and clean response\n        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = full_response.split(\"### Response:\\n\")[-1].strip()\n        \n        return {\n            \"response\": response,\n            \"generation_time\": f\"{(end_time - start_time):.2f} seconds\"\n        }\n    \n    def compare_with_base(self, instruction):\n        \"\"\"Compare responses from fine-tuned and base models\"\"\"\n        # Load base model for comparison\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        # Generate with fine-tuned model\n        ft_response = self.generate_response(instruction)\n        \n        # Generate with base model\n        temp_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        prompt = f\"### Instruction:\\n{instruction}\\n\\n### Response:\\n\"\n        inputs = temp_tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n        \n        with torch.no_grad():\n            outputs = base_model.generate(\n                **inputs,\n                max_length=256,\n                temperature=0.7,\n                top_p=0.9,\n                do_sample=True\n            )\n        \n        base_response = temp_tokenizer.decode(outputs[0], skip_special_tokens=True)\n        base_response = base_response.split(\"### Response:\\n\")[-1].strip()\n        \n        return {\n            \"instruction\": instruction,\n            \"fine_tuned_response\": ft_response[\"response\"],\n            \"base_model_response\": base_response\n        }\n\ndef run_tests():\n    # Replace these with your actual paths\n    base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    adapter_path = \"./finetuned_tinyllama_lora_20241110_1507\"  # Replace with your path\n    \n    print(\"\\n=== Initializing Model Testing ===\")\n    tester = LoRAModelTester(base_model_name, adapter_path)\n    \n    # Test cases covering different aspects\n    test_cases = [\n        # Basic instruction following\n        \"Explain what machine learning is in simple terms.\",\n        \"Write a short poem about autumn.\",\n        \n        # Complex reasoning\n        \"Compare and contrast supervised and unsupervised learning.\",\n        \"Explain the pros and cons of remote work.\",\n        \n        # Creative tasks\n        \"Write a short story about a robot discovering emotions.\",\n        \"Create a recipe for a healthy breakfast smoothie.\",\n        \n        # Analytical tasks\n        \"Analyze the impact of social media on modern society.\",\n        \"Describe the key factors that contribute to climate change.\"\n    ]\n    \n    # Run tests and save results\n    results = {\n        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"model_info\": {\n            \"base_model\": base_model_name,\n            \"adapter_path\": adapter_path,\n            \"device\": tester.device\n        },\n        \"test_results\": []\n    }\n    \n    print(\"\\n=== Running Test Cases ===\")\n    for i, test_case in enumerate(test_cases, 1):\n        print(f\"\\nTest Case {i}/{len(test_cases)}\")\n        print(f\"Prompt: {test_case}\")\n        \n        # Generate response\n        response = tester.generate_response(test_case)\n        print(f\"Response: {response['response']}\")\n        print(f\"Generation Time: {response['generation_time']}\")\n        \n        results[\"test_results\"].append({\n            \"prompt\": test_case,\n            \"response\": response[\"response\"],\n            \"generation_time\": response[\"generation_time\"]\n        })\n    \n    # Save results\n    output_file = f\"test_results_{datetime.now().strftime('%Y%m%d_%H%M')}.json\"\n    with open(output_file, 'w') as f:\n        json.dump(results, f, indent=2)\n    print(f\"\\nTest results saved to {output_file}\")\n    \n    # Interactive testing mode\n    print(\"\\n=== Starting Interactive Mode ===\")\n    print(\"Enter your prompts (type 'exit' to quit, 'compare' to compare with base model):\")\n    \n    while True:\n        user_input = input(\"\\nPrompt: \").strip()\n        \n        if user_input.lower() == 'exit':\n            break\n        elif user_input.lower() == 'compare':\n            compare_prompt = input(\"Enter prompt for comparison: \").strip()\n            comparison = tester.compare_with_base(compare_prompt)\n            print(\"\\n=== Model Comparison ===\")\n            print(f\"Prompt: {comparison['instruction']}\")\n            print(f\"\\nFine-tuned model response:\\n{comparison['fine_tuned_response']}\")\n            print(f\"\\nBase model response:\\n{comparison['base_model_response']}\")\n        else:\n            result = tester.generate_response(user_input)\n            print(f\"\\nResponse: {result['response']}\")\n            print(f\"Generation Time: {result['generation_time']}\")\n\nif __name__ == \"__main__\":\n    run_tests()","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:22:06.73157Z","iopub.execute_input":"2024-11-10T15:22:06.732026Z","iopub.status.idle":"2024-11-10T15:24:41.820769Z","shell.execute_reply.started":"2024-11-10T15:22:06.731985Z","shell.execute_reply":"2024-11-10T15:24:41.819929Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"\n=== Initializing Model Testing ===\nUsing device: cuda\nLoading tokenizer...\nLoading base model...\nLoading LoRA adapter...\nMerging weights for optimized inference...\nModel loading complete!\n\n=== Running Test Cases ===\n\nTest Case 1/8\nPrompt: Explain what machine learning is in simple terms.\nResponse: Machine learning is the field of artificial intelligence that allows machines to learn from data without being programmed explicitly. This field has been growing rapidly in recent years, with increasing demand for AI in various domains, such as healthcare, finance, and transportation. Machine learning is commonly used for tasks such as recommending products, predicting outcomes, and improving decision-making. In this context, machine learning can be applied to various fields, including healthcare, finance, and transportation.\n\nMachine learning is based on the concept of supervised learning, where the machine is trained on a dataset of examples, and the goal is to learn from the data how to classify or make predictions based on the data. The machine then uses the learned model to make predictions or classifications in new data that it has never seen before. This is called unsupervised learning, where the machine is not explicitly trained on a dataset, but learns the structure of the data by itself.\n\nMachine learning can also be applied to reinforcement learning, where the machine is trained on a reward system to learn how to navigate a game or solve\nGeneration Time: 6.14 seconds\n\nTest Case 2/8\nPrompt: Write a short poem about autumn.\nResponse: The leaves rustle in the wind,\nThe air is cool and crisp,\nThe world is at its best,\nThe colors are breathtaking.\n\nThe trees are turning golden,\nThe leaves are falling from the sky,\nThe forest is alive with music,\nAutumn's melody never ends.\n\nThe sun sets early,\nThe sky turns a brilliant shade,\nThe world is peaceful and still,\nAutumn's beauty is forever.\n\nThe leaves are crunching underfoot,\nThe sound is a symphony,\nThe world is changing,\nAutumn's arrival is a sight.\n\nThe leaves fall gently to the ground,\nThe trees are quiet and still,\nThe world is a new beginning,\nAutumn's arrival is a moment.\n\nThe air is cool and crisp,\nThe world is alive with music,\nAutumn's melody never ends,\nThe world is peaceful and still.\nGeneration Time: 5.32 seconds\n\nTest Case 3/8\nPrompt: Compare and contrast supervised and unsupervised learning.\nResponse: Supervised learning is a type of machine learning where the input and output of the model are predefined and known. The model is trained using data that contains labeled examples of the output. Unsupervised learning, on the other hand, does not require labeled data. Instead, the model learns the structure of the data without the need for ground truth labels. Unsupervised learning is useful for discovering relationships between the input data and the output.\n\nSupervised learning is useful for making predictions about the output of a model given labeled data. This is because the model can be trained on a set of labeled examples, which are known to be correct. In contrast, unsupervised learning is useful for discovering the structure of the data without knowing the correct output. This is because the model learns the structure of the data without being trained on labeled examples.\n\nIn summary, supervised learning is useful for making predictions about the output of a model given labeled data, while unsupervised learning is useful for discovering the structure of the data without knowing the correct output.\nGeneration Time: 5.68 seconds\n\nTest Case 4/8\nPrompt: Explain the pros and cons of remote work.\nResponse: Pros of remote work:\n- Flexibility: work from home, travel, vacation, etc.\n- Better work-life balance: working from home, no commute, more time with family\n- Cost savings: no rent, no car payments\n- Cost reduction: less office space, less equipment, less utilities\n- Better mental health: remote work eliminates the need to commute\n- Better work environment: remote work eliminates the need for a physical office\n- Better work-life balance: remote work eliminates the need for a physical office\n\nCons of remote work:\n- Lack of interaction with coworkers\n- Lack of social interaction\n- Lack of collaboration\n- Lack of office culture\n- Lack of company culture\n- Lack of feedback and support\n- Lack of training and development opportunities\n- Lack of team building activities\n- Lack of personal development opportunities\n- Lack of job security\n- Lack of autonomy\n- Lack of flexibility\n- Lack of job satisfaction\n- Lack of support\nGeneration Time: 5.83 seconds\n\nTest Case 5/8\nPrompt: Write a short story about a robot discovering emotions.\nResponse: One day, a robot named Max discovered that he had emotions. He had been programmed to do certain tasks, but he discovered that he felt emotions as well. He discovered that he was sad, happy, angry, and frightened. He discovered that he had a family, he had friends, and he loved them. He discovered that he had feelings, and he was not alone.\n\nMax decided to share his emotions with his family. He shared his sadness with his family. He shared his happiness with his family. He shared his anger with his family. He shared his fears with his family. He shared his love with his family.\n\nMax's family loved him more than anything in the world. They took care of him and loved him. Max realized that he was not alone, and he was not the only one who felt emotions. He realized that he was not the only one who had feelings.\n\nMax decided to use his emotions to help his family. He decided to share his emotions with his family in order to help them feel more empathy towards each other. He\nGeneration Time: 5.82 seconds\n\nTest Case 6/8\nPrompt: Create a recipe for a healthy breakfast smoothie.\nResponse: Here's a recipe for a healthy breakfast smoothie:\n\nIngredients:\n- 1 ripe banana\n- 1 cup frozen mixed berries\n- 1 cup unsweetened almond milk\n- 1 tablespoon chia seeds\n- 1 tablespoon ground flaxseed\n- 1 teaspoon vanilla extract\n- 1 cup spinach\n- 1 cup unsweetened almond milk\n- 1 tablespoon honey\n- 1 tablespoon chia seeds\n- 1 teaspoon ground flaxseed\n\nInstructions:\n1. Add the frozen mixed berries, almond milk, chia seeds, ground flaxseed, and vanilla extract to a blender.\n2. Add the spinach and almond milk.\n3. Blend until smooth and creamy.\n4. Add the honey and chia seeds.\n5. Blend again until well combined.\n6. Pour the smoothie into a glass\nGeneration Time: 5.82 seconds\n\nTest Case 7/8\nPrompt: Analyze the impact of social media on modern society.\nResponse: Social media has revolutionized modern society in ways that were unimaginable even a few decades ago.  The impact of social media on modern society has been significant in various ways.  First, social media has provided people with a way to communicate with people around the world in real-time.  This has allowed people to interact with others in a way that was not possible before.  Second, social media has provided people with a way to connect with others on a deeper level.  People can connect with others on social media by sharing their lives, their thoughts, and their experiences.  This has allowed people to feel more connected to others and to feel less lonely.  Third, social media has provided people with a way to connect with others who share their interests.  People can connect with others who share their interests on social media by following them on social media and engaging in conversations with them.  This has allowed people to feel more engaged with others and to feel like they are part of a community.  Fourth, social media has provided people with a way to connect with others on a more personal level.\nGeneration Time: 5.87 seconds\n\nTest Case 8/8\nPrompt: Describe the key factors that contribute to climate change.\nResponse: There are several key factors that contribute to climate change. These include:\n\n1. Greenhouse gases (GHGs) - most of the climate change is caused by the buildup of greenhouse gases such as carbon dioxide (CO2) in the atmosphere. The most common GHGs are carbon dioxide, methane, nitrous oxide, and fluorinated gases. These gases trap heat in the atmosphere and lead to warmer temperatures.\n\n2. Human activities - human activities such as burning fossil fuels, deforestation, and agriculture are responsible for releasing GHGs into the atmosphere.\n\n3. Natural climate cycles - natural climate cycles such as El Niño and La Niña can also contribute to climate change. These cycles are caused by changes in ocean currents and sea level.\n\n4. Climate models - climate models can help us understand how climate change is likely to occur and what impacts it will have.\n\n5. Policy - policy can help reduce GHG emissions and\nGeneration Time: 5.81 seconds\n\nTest results saved to test_results_20241110_1522.json\n\n=== Starting Interactive Mode ===\nEnter your prompts (type 'exit' to quit, 'compare' to compare with base model):\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nPrompt:  Write a speech on Children's Day\n"},{"name":"stdout","text":"\nResponse: Children's Day is celebrated on the 14th of December every year in India. The day is celebrated to recognize the contribution of children towards the nation. On this day, the nation expresses gratitude towards children for their contributions to society. Children's Day is a national holiday in India. It is celebrated with great enthusiasm and enthusiasm among children and their families. The celebration of Children's Day in India is celebrated with great fervour and enthusiasm. The day is celebrated with various activities and events. One of the most important events on Children's Day is the distribution of sweets to children. Children are given sweets as a symbol of gratitude towards them. Children's Day is also celebrated with various other events like sports, cultural programs, and competitions. Children's Day is an excellent opportunity for children to showcase their talents and skills. Children's Day is celebrated with great enthusiasm and enthusiasm in various parts of India. The celebration of Children's Day is an excellent opportunity for children to learn about the importance of their contribution to society. Children'\nGeneration Time: 5.88 seconds\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nPrompt:  Childrens Day is celebrated on 14th November Every Year in India\n"},{"name":"stdout","text":"\nResponse: Childrens Day is celebrated on the 14th of November in India as it is the birthday of Sri Aurobindo. The day is celebrated with great enthusiasm and celebrations all over the country. There are various activities organized on this day like kite flying, singing, painting, dance, etc. Kids from schools and colleges participate in these activities and celebrate the day.\n\nChildrens Day is celebrated as a day to celebrate the children and their happiness. Children are treated like celebrities and given a day off from school and other academic activities. The day is dedicated to the children and their happiness and is celebrated in a very different way in each part of the country.\n\nThe celebration of Childrens Day is very popular in Kerala, Karnataka, Tamil Nadu and Andhra Pradesh. These states celebrate Childrens Day as the birthday of their respective state leaders. The day is celebrated with great pomp and show and is a very colorful and vibrant day in these states.\nGeneration Time: 5.81 seconds\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nPrompt:  exit\n"}]},{"cell_type":"markdown","source":"# Simple Inference on the Fine-Tuned Model","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nclass SimpleInference:\n    def __init__(self, base_model_name, adapter_path):\n        \"\"\"Initialize model with base model and LoRA adapter\"\"\"\n        print(\"Loading model...\")\n        \n        # Load tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n        \n        # Load base model\n        base_model = AutoModelForCausalLM.from_pretrained(\n            base_model_name,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        )\n        \n        # Load and merge LoRA adapter\n        self.model = PeftModel.from_pretrained(\n            base_model,\n            adapter_path,\n            torch_dtype=torch.float16,\n            device_map=\"auto\"\n        ).merge_and_unload()\n        \n        print(\"Model ready!\")\n    \n    def generate(self, prompt, max_length=256):\n        \"\"\"Generate response for given prompt\"\"\"\n        # Format prompt\n        formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n        \n        # Tokenize\n        inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(self.model.device)\n        \n        # Generate\n        outputs = self.model.generate(\n            **inputs,\n            max_length=max_length,\n            temperature=0.7,\n            top_p=0.9,\n            do_sample=True\n        )\n        \n        # Decode and clean response\n        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n        response = response.split(\"### Response:\\n\")[-1].strip()\n        \n        return response\n\n# Main execution\n\n# Replace these with your actual paths\nbase_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\nadapter_path = \"./finetuned_tinyllama_lora_20241110_1507\"  # Replace with your path\n\nif __name__ == \"__main__\":\n    # Initialize model\n    generator = SimpleInference(\n        base_model_name=base_model_name,\n        adapter_path=adapter_path  # Replace with your model path\n    )\n    \n    print(\"\\nChat with your fine-tuned model (type 'exit' to quit)\")\n    print(\"-\" * 50)\n    \n    while True:\n        user_input = input(\"\\nYou: \").strip()\n        \n        if user_input.lower() == 'exit':\n            print(\"Goodbye!\")\n            break\n            \n        response = generator.generate(user_input)\n        print(\"\\nModel:\", response)\n        print(\"-\" * 50)","metadata":{"execution":{"iopub.status.busy":"2024-11-10T15:28:37.77407Z","iopub.execute_input":"2024-11-10T15:28:37.774889Z","iopub.status.idle":"2024-11-10T15:30:25.674138Z","shell.execute_reply.started":"2024-11-10T15:28:37.774843Z","shell.execute_reply":"2024-11-10T15:30:25.673133Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Loading model...\nModel ready!\n\nChat with your fine-tuned model (type 'exit' to quit)\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  Hi\n"},{"name":"stdout","text":"\nModel: I am in the US and have a dog. My dog is a Chihuahua. It's a great dog. It is small and loves to play with my kids. It's a fun dog to have around. My dog is very friendly and loves to cuddle. It's a great dog to have around. It's always a good idea to have a dog. A dog is a great addition to any family. My dog is a Chihuahua. It's a great dog. It's very friendly and loves to play with my kids. It's a great dog to have around. It's a great addition to any family. My dog is a Chihuahua. It's a great dog. It's very friendly and loves to play with my kids. It's a great dog to have around. It's a great addition to any family. My dog is a Chihuahua. It's a great dog. It's very friendly and loves to play with my kids. It's a great dog to have around.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  What is your specialization? \n"},{"name":"stdout","text":"\nModel: A logo is the visual representation of a brand. It is the visual representation of the brand's name, slogan, tagline, or image. The logo is the first thing people see when they encounter a brand. The logo is the face of the brand.\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  what is fine-tuning a model? \n"},{"name":"stdout","text":"\nModel: Fine-tuning is a process where you modify a pre-trained model to make it better suited to a specific task. The model is pre-trained on a large dataset and then fine-tuned to perform a specific task. The main idea is to learn the parameters of the pre-trained model on the specific task and then re-train the model with new data. Fine-tuning is an important technique in many deep learning models because it allows you to learn from a large dataset and to make the model more specific to the task at hand. The process of fine-tuning is usually done in two steps: pre-training and fine-tuning. \n\nIn the first step, the model is pre-trained on a large dataset. This step is typically done by training a model on a dataset that has been pre-processed and transformed into a format that can be used by the model. This usually involves converting the dataset into a format that the model can understand, such as images, text, or audio. Once the model is pre-trained, it is then fine-tuned\n--------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\nYou:  exit\n"},{"name":"stdout","text":"Goodbye!\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}