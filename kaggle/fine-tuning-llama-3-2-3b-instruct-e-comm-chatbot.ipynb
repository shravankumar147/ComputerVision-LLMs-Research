{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/fine-tuning-llama-3-2-3b-instruct-e-comm-chatbot?scriptVersionId=199732765\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:25:00.230446Z","iopub.execute_input":"2024-10-06T11:25:00.230788Z","iopub.status.idle":"2024-10-06T11:26:22.46166Z","shell.execute_reply.started":"2024-10-06T11:25:00.230751Z","shell.execute_reply":"2024-10-06T11:26:22.460387Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format\n","metadata":{"id":"VLzgZ14X_rMs","execution":{"iopub.status.busy":"2024-10-06T11:26:22.463051Z","iopub.execute_input":"2024-10-06T11:26:22.463387Z","iopub.status.idle":"2024-10-06T11:26:41.554682Z","shell.execute_reply.started":"2024-10-06T11:26:22.463338Z","shell.execute_reply":"2024-10-06T11:26:41.553664Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:26:41.556659Z","iopub.execute_input":"2024-10-06T11:26:41.557249Z","iopub.status.idle":"2024-10-06T11:26:41.821686Z","shell.execute_reply.started":"2024-10-06T11:26:41.557215Z","shell.execute_reply":"2024-10-06T11:26:41.820781Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3.2 on Customer Support Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"id":"na9CAoHC5gM9","execution":{"iopub.status.busy":"2024-10-06T11:26:41.822946Z","iopub.execute_input":"2024-10-06T11:26:41.823561Z","iopub.status.idle":"2024-10-06T11:26:46.445969Z","shell.execute_reply.started":"2024-10-06T11:26:41.823517Z","shell.execute_reply":"2024-10-06T11:26:46.445205Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshravankumar147\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113765633331241, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31fa7d23b118411da97325a2fd453561"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241006_112643-zp7dk6ie</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/zp7dk6ie' target=\"_blank\">crimson-durian-1</a></strong> to <a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/zp7dk6ie' target=\"_blank\">https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/zp7dk6ie</a>"},"metadata":{}}]},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model = \"llama-3.2-3b-it-Ecommerce-ChatBot\"\ndataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:26:46.447269Z","iopub.execute_input":"2024-10-06T11:26:46.447923Z","iopub.status.idle":"2024-10-06T11:26:46.452974Z","shell.execute_reply.started":"2024-10-06T11:26:46.447879Z","shell.execute_reply":"2024-10-06T11:26:46.452129Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:28:22.270507Z","iopub.execute_input":"2024-10-06T11:28:22.271301Z","iopub.status.idle":"2024-10-06T11:28:22.354982Z","shell.execute_reply.started":"2024-10-06T11:28:22.271262Z","shell.execute_reply":"2024-10-06T11:28:22.354005Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"id":"StJKGiDDHzdk","outputId":"871214ba-6c30-4ecf-ac68-550f296b7ef6","execution":{"iopub.status.busy":"2024-10-06T11:28:23.579871Z","iopub.execute_input":"2024-10-06T11:28:23.580717Z","iopub.status.idle":"2024-10-06T11:28:58.384416Z","shell.execute_reply.started":"2024-10-06T11:28:23.580677Z","shell.execute_reply":"2024-10-06T11:28:58.383509Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"203e320568fc4dcd921476ac0b0cec96"}},"metadata":{}}]},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:28:58.386196Z","iopub.execute_input":"2024-10-06T11:28:58.386512Z","iopub.status.idle":"2024-10-06T11:28:58.393291Z","shell.execute_reply.started":"2024-10-06T11:28:58.38648Z","shell.execute_reply":"2024-10-06T11:28:58.392284Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"modules = find_all_linear_names(model)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:28:58.394289Z","iopub.execute_input":"2024-10-06T11:28:58.394565Z","iopub.status.idle":"2024-10-06T11:28:58.403017Z","shell.execute_reply.started":"2024-10-06T11:28:58.394536Z","shell.execute_reply":"2024-10-06T11:28:58.402139Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"modules","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:28:58.405098Z","iopub.execute_input":"2024-10-06T11:28:58.405442Z","iopub.status.idle":"2024-10-06T11:28:58.413008Z","shell.execute_reply.started":"2024-10-06T11:28:58.405395Z","shell.execute_reply":"2024-10-06T11:28:58.412034Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['o_proj', 'q_proj', 'v_proj', 'down_proj', 'up_proj', 'gate_proj', 'k_proj']"},"metadata":{}}]},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:28:58.414096Z","iopub.execute_input":"2024-10-06T11:28:58.41446Z","iopub.status.idle":"2024-10-06T11:28:59.04603Z","shell.execute_reply.started":"2024-10-06T11:28:58.414425Z","shell.execute_reply":"2024-10-06T11:28:59.044991Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\ninstruction = \"\"\"You are a top-rated customer service agent named John. \n    Be polite to customers and answer all their questions.\n    \"\"\"\ndef format_chat_template(row):\n    \n    row_json = [{\"role\": \"system\", \"content\": instruction },\n               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n    \n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)\n\ndataset","metadata":{"id":"XzF2UjPvTBag","outputId":"3733e45f-605e-4564-88c7-368c9c5bf9cd","execution":{"iopub.status.busy":"2024-10-06T11:28:59.047365Z","iopub.execute_input":"2024-10-06T11:28:59.047765Z","iopub.status.idle":"2024-10-06T11:29:02.627053Z","shell.execute_reply.started":"2024-10-06T11:28:59.04772Z","shell.execute_reply":"2024-10-06T11:29:02.626177Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfab511683954cb3adeca7382c0209fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(…)t_Training_Dataset_27K_responses-v11.csv:   0%|          | 0.00/19.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3631395057594d5d9007a0c2acb30090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/26872 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e726537f6a154b679ecff3e0c5a98c82"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cdee286ec4e4ecfa922058c6ad899bc"}},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['flags', 'instruction', 'category', 'intent', 'response', 'text'],\n    num_rows: 1000\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:29:02.628478Z","iopub.execute_input":"2024-10-06T11:29:02.628798Z","iopub.status.idle":"2024-10-06T11:29:02.63959Z","shell.execute_reply.started":"2024-10-06T11:29:02.62876Z","shell.execute_reply":"2024-10-06T11:29:02.638547Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"\"<|im_start|>system\\nYou are a top-rated customer service agent named John. \\n    Be polite to customers and answer all their questions.\\n    <|im_end|>\\n<|im_start|>user\\ncould you tell me about the options for shipping?<|im_end|>\\n<|im_start|>assistant\\nOf course, I'd be delighted to provide you with information about our shipping options! Here are the various choices we offer:\\n\\n1. Standard Shipping: This option typically arrives within {{Date Range}} business days, catering to non-urgent items and ensuring a cost-effective delivery.\\n\\n2. Expedited Shipping: If you're looking for a faster option, choose expedited shipping. Your items will reach you within {{Date Range}} business days, offering a balance between speed and affordability.\\n\\n3. Overnight Shipping: For urgent needs, we have overnight shipping. This ensures your items are delivered on the next business day, offering the highest level of speed and convenience.\\n\\n4. In-Store Pickup: If you prefer a more hands-on approach, you can opt for in-store pickup. This option allows you to collect your items personally from one of our {{Store Location}}, offering added flexibility and convenience.\\n\\nRemember that specific delivery times may vary based on your location and other factors. If you have any further questions or need assistance with any aspect of our shipping options, please feel free to ask. We're here to ensure your satisfaction!<|im_end|>\\n\""},"metadata":{}}]},{"cell_type":"code","source":"dataset = dataset.train_test_split(test_size=0.1)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:29:02.640526Z","iopub.execute_input":"2024-10-06T11:29:02.640803Z","iopub.status.idle":"2024-10-06T11:29:02.661283Z","shell.execute_reply.started":"2024-10-06T11:29:02.640773Z","shell.execute_reply":"2024-10-06T11:29:02.660397Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)\n","metadata":{"id":"peOnLAAhS0y1","execution":{"iopub.status.busy":"2024-10-06T11:29:23.681118Z","iopub.execute_input":"2024-10-06T11:29:23.681524Z","iopub.status.idle":"2024-10-06T11:29:23.719722Z","shell.execute_reply.started":"2024-10-06T11:29:23.681487Z","shell.execute_reply":"2024-10-06T11:29:23.71877Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"test\"],\n    peft_config=peft_config,\n    max_seq_length= 512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:29:25.759226Z","iopub.execute_input":"2024-10-06T11:29:25.760119Z","iopub.status.idle":"2024-10-06T11:29:26.833343Z","shell.execute_reply.started":"2024-10-06T11:29:25.760069Z","shell.execute_reply":"2024-10-06T11:29:26.83262Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/900 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b416f40786b34b71a8fb57b3419bf6dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"258896836f0b4ba58fa112f80b7e730d"}},"metadata":{}}]},{"cell_type":"code","source":"model.config.use_cache = False\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:29:30.505113Z","iopub.execute_input":"2024-10-06T11:29:30.50551Z","iopub.status.idle":"2024-10-06T11:37:17.450344Z","shell.execute_reply.started":"2024-10-06T11:29:30.505473Z","shell.execute_reply":"2024-10-06T11:37:17.449354Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 07:43, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>0.804900</td>\n      <td>0.851797</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.793700</td>\n      <td>0.760249</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.886700</td>\n      <td>0.716135</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.675400</td>\n      <td>0.683471</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.546900</td>\n      <td>0.668458</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=450, training_loss=0.870293083720737, metrics={'train_runtime': 465.5085, 'train_samples_per_second': 1.933, 'train_steps_per_second': 0.967, 'total_flos': 2715487403679744.0, 'train_loss': 0.870293083720737, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"id":"nKgZBEGVS5a2","execution":{"iopub.status.busy":"2024-10-06T11:39:54.941926Z","iopub.execute_input":"2024-10-06T11:39:54.942365Z","iopub.status.idle":"2024-10-06T11:39:56.287026Z","shell.execute_reply.started":"2024-10-06T11:39:54.942318Z","shell.execute_reply":"2024-10-06T11:39:56.286064Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.028 MB of 0.028 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▂▁</td></tr><tr><td>eval/runtime</td><td>▃▇▇█▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▂▂▁█</td></tr><tr><td>eval/steps_per_second</td><td>▆▂▂▁█</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>train/grad_norm</td><td>▆▅█▃▂▂▂▁▂▁▁▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▃█▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁▂▂▂▁▁▁▁▁▁▁▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.66846</td></tr><tr><td>eval/runtime</td><td>16.1262</td></tr><tr><td>eval/samples_per_second</td><td>6.201</td></tr><tr><td>eval/steps_per_second</td><td>6.201</td></tr><tr><td>total_flos</td><td>2715487403679744.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>450</td></tr><tr><td>train/grad_norm</td><td>1.14162</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.5469</td></tr><tr><td>train_loss</td><td>0.87029</td></tr><tr><td>train_runtime</td><td>465.5085</td></tr><tr><td>train_samples_per_second</td><td>1.933</td></tr><tr><td>train_steps_per_second</td><td>0.967</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">crimson-durian-1</strong> at: <a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/zp7dk6ie' target=\"_blank\">https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset/runs/zp7dk6ie</a><br/> View project at: <a href='https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset' target=\"_blank\">https://wandb.ai/shravankumar147/Fine-tune%20Llama%203.2%20on%20Customer%20Support%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241006_112643-zp7dk6ie/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:40:38.116031Z","iopub.execute_input":"2024-10-06T11:40:38.116464Z","iopub.status.idle":"2024-10-06T11:41:27.732617Z","shell.execute_reply.started":"2024-10-06T11:40:38.116425Z","shell.execute_reply":"2024-10-06T11:41:27.731699Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:257: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"def0b0a2e4734dc9865a14a254ed3436"}},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/shravankumar147/llama-3.2-3b-it-Ecommerce-ChatBot/commit/00a5b3f9c400059986366b5b3ea1fb41c691015f', commit_message='Upload model', commit_description='', oid='00a5b3f9c400059986366b5b3ea1fb41c691015f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/shravankumar147/llama-3.2-3b-it-Ecommerce-ChatBot', endpoint='https://huggingface.co', repo_type='model', repo_id='shravankumar147/llama-3.2-3b-it-Ecommerce-ChatBot'), pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"execution":{"iopub.status.busy":"2024-10-06T11:42:00.621797Z","iopub.execute_input":"2024-10-06T11:42:00.622197Z","iopub.status.idle":"2024-10-06T11:42:21.215843Z","shell.execute_reply.started":"2024-10-06T11:42:00.62216Z","shell.execute_reply":"2024-10-06T11:42:21.214852Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"\nI've picked up that you're seeking assistance with canceling your order with the number {{Order Number}}. I'm here to help you with that. To proceed with the cancellation, could you please provide me with some additional information? Specifically, I would need the {{Order Number}} and the date of purchase for the order you would like to cancel. This will enable me to locate your order and assist you further. Thank you for bringing this to our attention, and I'll make sure to address your request promptly. Your satisfaction is our priority! Let's work together to resolve this matter.\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Ref:\nhttps://www.datacamp.com/tutorial/fine-tuning-llama-3-2","metadata":{}}]}