{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/rag-with-deepseek-r1?scriptVersionId=219095960\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Install required packages\n!pip install -q pymupdf langchain faiss-cpu pandas transformers torch sentence-transformers accelerate bitsandbytes wget","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:16:14.347295Z","iopub.execute_input":"2025-01-24T15:16:14.347518Z","iopub.status.idle":"2025-01-24T15:16:26.350011Z","shell.execute_reply.started":"2025-01-24T15:16:14.347497Z","shell.execute_reply":"2025-01-24T15:16:26.34885Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -U langchain-community -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:18:19.101701Z","iopub.execute_input":"2025-01-24T15:18:19.102072Z","iopub.status.idle":"2025-01-24T15:18:27.858337Z","shell.execute_reply.started":"2025-01-24T15:18:19.102045Z","shell.execute_reply":"2025-01-24T15:18:27.857504Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!!pip install camelot-py[cv] -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:23:21.790953Z","iopub.execute_input":"2025-01-24T15:23:21.791309Z","iopub.status.idle":"2025-01-24T15:23:26.501747Z","shell.execute_reply.started":"2025-01-24T15:23:21.791279Z","shell.execute_reply":"2025-01-24T15:23:26.500989Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"[\"WARNING: camelot-py 1.0.0 does not provide the extra 'cv'\",\n '     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.2/48.2 kB 1.8 MB/s eta 0:00:00',\n '   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 64.1 MB/s eta 0:00:00',\n '   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 278.2/278.2 kB 17.4 MB/s eta 0:00:00',\n '   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 73.1 MB/s eta 0:00:00',\n '   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.6/66.6 kB 3.9 MB/s eta 0:00:00']"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\nimport fitz\nimport pandas as pd\nimport torch\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.docstore.document import Document\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:18:31.369492Z","iopub.execute_input":"2025-01-24T15:18:31.36985Z","iopub.status.idle":"2025-01-24T15:18:47.921751Z","shell.execute_reply.started":"2025-01-24T15:18:31.369818Z","shell.execute_reply":"2025-01-24T15:18:47.921082Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# https://arxiv.org/pdf/2305.10403","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Download the PDF\nimport wget\npdf_url = \"https://arxiv.org/pdf/2305.10403.pdf\"\npdf_filename = wget.download(pdf_url)\nprint(f\"\\nDownloaded: {pdf_filename}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:16:36.717054Z","iopub.execute_input":"2025-01-24T15:16:36.717386Z","iopub.status.idle":"2025-01-24T15:16:37.041899Z","shell.execute_reply.started":"2025-01-24T15:16:36.717352Z","shell.execute_reply":"2025-01-24T15:16:37.041259Z"}},"outputs":[{"name":"stdout","text":"\nDownloaded: 2305.10403v3.pdf\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Verify the download\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:16:56.096123Z","iopub.execute_input":"2025-01-24T15:16:56.096395Z","iopub.status.idle":"2025-01-24T15:16:56.214903Z","shell.execute_reply.started":"2025-01-24T15:16:56.096372Z","shell.execute_reply":"2025-01-24T15:16:56.214028Z"}},"outputs":[{"name":"stdout","text":"2305.10403v3.pdf\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndef extract_tables_with_context(pdf_path, context_window=300):\n    \"\"\"Extract tables with surrounding context from PDF\"\"\"\n    doc = fitz.open(pdf_path)\n    table_data = []\n    \n    for page_num in range(len(doc)):\n        page = doc.load_page(page_num)\n        text = page.get_text(\"text\")\n        tables = page.get_tables()\n        \n        for table_num, table in enumerate(tables):\n            try:\n                # Extract table as DataFrame\n                df = pd.DataFrame(table[1:], columns=table[0])\n                \n                # Get surrounding context\n                table_bbox = page.search_for(\" \".join(table[0][:3]))[0]\n                y0 = max(0, table_bbox.y0 - context_window)\n                context = page.get_text(\"text\", clip=(0, y0, page.rect.width, table_bbox.y0))\n                \n                table_data.append({\n                    \"page\": page_num + 1,\n                    \"table_num\": table_num + 1,\n                    \"context\": context.strip(),\n                    \"table\": df.to_markdown(index=False),\n                    \"full_text\": f\"Context: {context}\\nTable:\\n{df.to_markdown(index=False)}\"\n                })\n            except Exception as e:\n                print(f\"Error on page {page_num} table {table_num}: {str(e)[:50]}\")\n    \n    return table_data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:19:52.677933Z","iopub.execute_input":"2025-01-24T15:19:52.678611Z","iopub.status.idle":"2025-01-24T15:19:52.684927Z","shell.execute_reply.started":"2025-01-24T15:19:52.67858Z","shell.execute_reply":"2025-01-24T15:19:52.684093Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import camelot\n\ndef extract_tables_with_context(pdf_path, context_window=300):\n    \"\"\"Extract tables using camelot\"\"\"\n    tables = camelot.read_pdf(pdf_path, pages=\"all\", flavor=\"lattice\")\n    table_data = []\n    \n    for i, table in enumerate(tables):\n        try:\n            df = table.df\n            table_data.append({\n                \"page\": table.page,\n                \"table_num\": i + 1,\n                \"context\": \"\",  # Camelot doesn't extract context\n                \"table\": df.to_markdown(index=False),\n                \"full_text\": f\"Table:\\n{df.to_markdown(index=False)}\"\n            })\n        except Exception as e:\n            print(f\"Error processing table {i + 1}: {e}\")\n    \n    return table_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:24:10.696828Z","iopub.execute_input":"2025-01-24T15:24:10.697133Z","iopub.status.idle":"2025-01-24T15:24:11.07106Z","shell.execute_reply.started":"2025-01-24T15:24:10.697111Z","shell.execute_reply":"2025-01-24T15:24:11.070322Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pypdf/_crypt_providers/_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndef create_vector_store(table_data):\n    \"\"\"Create FAISS vector store with efficient embeddings\"\"\"\n    embeddings = HuggingFaceEmbeddings(\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        model_kwargs={'device': 'cuda' if torch.cuda.is_available() else 'cpu'},\n        encode_kwargs={'normalize_embeddings': False}\n    )\n    \n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200,\n        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n    )\n    \n    docs = []\n    for entry in table_data:\n        chunks = text_splitter.split_text(entry[\"full_text\"])\n        for chunk in chunks:\n            docs.append(Document(\n                page_content=chunk,\n                metadata={\n                    \"page\": entry[\"page\"],\n                    \"table_num\": entry[\"table_num\"],\n                    \"context\": entry[\"context\"]\n                }\n            ))\n    \n    return FAISS.from_documents(docs, embeddings)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:19:57.894963Z","iopub.execute_input":"2025-01-24T15:19:57.895304Z","iopub.status.idle":"2025-01-24T15:19:57.900656Z","shell.execute_reply.started":"2025-01-24T15:19:57.895273Z","shell.execute_reply":"2025-01-24T15:19:57.899601Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# # %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# def load_deepseek_model():\n#     \"\"\"Load quantized DeepSeek-R1 model\"\"\"\n#     # model_name = \"deepseek-ai/deepseek-r1\"\n#     # model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n#     model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n    \n#     tokenizer = AutoTokenizer.from_pretrained(model_name)\n#     model = AutoModelForCausalLM.from_pretrained(\n#         model_name,\n#         device_map=\"auto\",\n#         load_in_4bit=True,\n#         torch_dtype=torch.float16,\n#         low_cpu_mem_usage=True\n#     )\n    \n#     return pipeline(\n#         \"text-generation\",\n#         model=model,\n#         tokenizer=tokenizer,\n#         max_new_tokens=512,\n#         temperature=0.1,\n#         top_p=0.9,\n#         repetition_penalty=1.1\n#     )","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:41:54.742669Z","iopub.execute_input":"2025-01-24T15:41:54.743048Z","iopub.status.idle":"2025-01-24T15:41:54.747876Z","shell.execute_reply.started":"2025-01-24T15:41:54.743017Z","shell.execute_reply":"2025-01-24T15:41:54.747042Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndef load_deepseek_model():\n    \"\"\"Load quantized DeepSeek-R1 model using BitsAndBytesConfig\"\"\"\n    # model_name = \"deepseek-ai/deepseek-r1\"\n    model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n    \n    # Define quantization configuration\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,  # Use 4-bit quantization\n        bnb_4bit_use_double_quant=True,  # Nested quantization for better accuracy\n        bnb_4bit_quant_type=\"nf4\",  # Use 4-bit NormalFloat quantization\n        bnb_4bit_compute_dtype=torch.float16  # Compute dtype for 4-bit\n    )\n    \n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        device_map=\"auto\",  # Use GPU if available\n        quantization_config=quantization_config,  # Pass quantization config\n        torch_dtype=torch.float16,\n        low_cpu_mem_usage=True\n    )\n    \n    return pipeline(\n        \"text-generation\",\n        model=model,\n        tokenizer=tokenizer,\n        max_new_tokens=512,\n        temperature=0.1,\n        top_p=0.9,\n        repetition_penalty=1.1\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\ndef query_tables(vector_store, query, llm_pipeline, k=5):\n    \"\"\"Query tables using DeepSeek-R1\"\"\"\n    docs = vector_store.similarity_search(query, k=k)\n    context = \"\\n\\n\".join([f\"Page {doc.metadata['page']} Table {doc.metadata['table_num']}:\\n{doc.page_content}\" \n                          for doc in docs])\n    \n    prompt = f\"\"\"Analyze these PDF table extracts and answer the query:\n    \n    {context}\n    \n    Query: {query}\n    \n    Format your answer with:\n    1. Relevant tables identified by page and table number\n    2. Brief summary of each relevant table\n    3. Answer to the query based on tables\n    \n    Response:\"\"\"\n    \n    response = llm_pipeline(prompt)[0]['generated_text']\n    return {\n        \"answer\": response.split(\"Response:\")[-1].strip(),\n        \"references\": [\n            {\n                \"page\": doc.metadata[\"page\"],\n                \"table_num\": doc.metadata[\"table_num\"],\n                \"content\": doc.page_content[:500] + \"...\"\n            } for doc in docs\n        ]\n    }","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:20:07.752442Z","iopub.execute_input":"2025-01-24T15:20:07.752737Z","iopub.status.idle":"2025-01-24T15:20:07.757937Z","shell.execute_reply.started":"2025-01-24T15:20:07.752713Z","shell.execute_reply":"2025-01-24T15:20:07.757145Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# # %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# # Main Execution\n# if __name__ == \"__main__\":\n#     # Upload your PDF first in Colab/Kaggle\n#     PDF_PATH = \"/kaggle/working/2305.10403v3.pdf\"\n    \n#     # 1. Extract tables\n#     print(\"Extracting tables...\")\n#     tables = extract_tables_with_context(PDF_PATH)\n#     print(f\"Found {len(tables)} tables\")\n    \n#     # 2. Create vector store\n#     print(\"Creating vector database...\")\n#     vector_store = create_vector_store(tables)\n    \n#     # 3. Load DeepSeek\n#     print(\"Loading DeepSeek-R1 model...\")\n#     deepseek_pipe = load_deepseek_model()\n    \n#     # 4. Example query\n#     query = \"Evaluation results on MATH, GSM8K, and MGSM\"\n#     print(\"\\nProcessing query:\", query)\n    \n#     results = query_tables(vector_store, query, deepseek_pipe)\n    \n#     print(\"\\nFinal Answer:\")\n#     print(results[\"answer\"])\n    \n#     print(\"\\nReference Tables:\")\n#     for ref in results[\"references\"]:\n#         print(f\"\\nPage {ref['page']} | Table {ref['table_num']}\")\n#         print(ref[\"content\"])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:29:28.974141Z","iopub.execute_input":"2025-01-24T15:29:28.97493Z","iopub.status.idle":"2025-01-24T15:31:12.255249Z","shell.execute_reply.started":"2025-01-24T15:29:28.974892Z","shell.execute_reply":"2025-01-24T15:31:12.254317Z"}},"outputs":[{"name":"stdout","text":"Extracting tables...\nFound 10 tables\nCreating vector database...\nLoading DeepSeek-R1 model...\n","output_type":"stream"},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"\nProcessing query: Evaluation results on MATH, GSM8K, and MGSM\n\nFinal Answer:\nPlease provide a detailed analysis of the PDF table extracts and answer the query.\nAlright, so I need to analyze these PDF tables related to machine learning models, specifically focusing on how they're being evaluated. The user has provided three tables: one for \"MATH,\" another for \"GSM8K,\" and another for \"MGSM.\" Each table seems to detail different aspects like impact on real people, scoring mechanisms, cultural diversity, etc.\n\nFirst, let me understand what each table represents. Looking at the first table, which is labeled \"Table 10\" on page 84, it's about whether LLMs (large language models) are used in certain products. It mentions impacts on real people, scoring, cultural diversity, and some technical details about how these models are evaluated. \n\nThe second table, \"GSM8K,\" also on page 83, seems to focus more on specific tasks like toxicity classification. It talks about evaluating data synth, chatbot, etc., and mentions things like multilinguality and resource limitations. This could relate to how well models handle diverse data sets.\n\nThe third table, \"MGSM,\" on page 83, looks similar to the previous ones but perhaps focuses on different aspects, maybe more on implementation details or specific evaluation metrics. However, without seeing the exact content, it's hard to say.\n\nNow, moving on to the query: Evaluation results on MATH, GSM8K, and MGSM. So, I need to see how these tables relate to each of these evaluation tasks.\n\nStarting with MATH, which stands for Math-Aware Generation of Text. I think this refers to how models generate mathematical expressions or equations. If the tables mention scoring and cultural diversity, that might tie into how well models perform in math-related tasks.\n\nLooking at the first table, \"MATH,\" on page 84, it discusses whether LLMs are used in product lines and their impact on real people. It mentions scoring and cultural diversity. Maybe this table is part of the MATH evaluation because it talks about scoring and possibly how models perform in generating mathematical content.\n\nNext, GSM8K is a dataset used for text generation tasks. It's often used in benchmarks where models are evaluated on producing coherent text. The tables here discuss scoring, signals, noise, and evaluation bounds. Perhaps this relates to how well models score against GSM8K data, affecting their ability to generate realistic text.\n\nLastly, MGSM—maybe it's another benchmark or evaluation setup. Without seeing\n\nReference Tables:\n\nPage 84 | Table 10\n| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.   | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                               |                                      |                                   | systems.             ...\n\nPage 84 | Table 10\n| zero-shot anticipates         | Constructed from real                | into English.                     | Automated scoring                    | Can be visualized directly         |\n| potential translation         | world datasets, aligned with         | Automated scoring allows          | methods and signals are              | during training.                   |\n| capabilities of future LLMs.  | harms in real systems.               | using different eval set          | reliable and determin...\n\nPage 83 | Table 9\n| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.   | signals are separately validated.    | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                               |                                      |                                   | systems.             ...\n\nPage 83 | Table 8\n| LLM might be used by product    | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.     | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                                 |                                      |                                   | systems.       ...\n\nPage 83 | Table 8\n| With potential mitigations:     | Approximate real usage or            | development and change            | model checkpoints.                   | Documentation and tooling          |\n| eg, filters, sample and rank    | input distributions (eg, not         | over time.                        |                                      | for interpreting results (eg,      |\n|                                 | only artificially controlled         |                                   |                ...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# %% [code] {\"jupyter\":{\"outputs_hidden\":false}}\n# Main Execution\n# if __name__ == \"__main__\":\n# Upload your PDF first in Colab/Kaggle\nPDF_PATH = \"/kaggle/working/2305.10403v3.pdf\"\n\n# 1. Extract tables\nprint(\"Extracting tables...\")\ntables = extract_tables_with_context(PDF_PATH)\nprint(f\"Found {len(tables)} tables\")\n\n# 2. Create vector store\nprint(\"Creating vector database...\")\nvector_store = create_vector_store(tables)\nvector_store.save_local(\"vector_store\")\nprint(\"Vector store saved to 'vector_store' directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:36:22.795145Z","iopub.execute_input":"2025-01-24T15:36:22.795471Z","iopub.status.idle":"2025-01-24T15:37:37.504656Z","shell.execute_reply.started":"2025-01-24T15:36:22.795445Z","shell.execute_reply":"2025-01-24T15:37:37.503881Z"}},"outputs":[{"name":"stdout","text":"Extracting tables...\nFound 10 tables\nCreating vector database...\nVector store saved to 'vector_store' directory.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# 3. Load DeepSeek\nprint(\"Loading DeepSeek-R1 model...\")\ndeepseek_pipe = load_deepseek_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T15:42:04.691535Z","iopub.execute_input":"2025-01-24T15:42:04.691883Z","iopub.status.idle":"2025-01-24T15:49:48.98761Z","shell.execute_reply.started":"2025-01-24T15:42:04.691851Z","shell.execute_reply":"2025-01-24T15:49:48.986946Z"}},"outputs":[{"name":"stdout","text":"Loading DeepSeek-R1 model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cefc64406604cd1a2b08ad4b9351031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d7a3bf6db89496f82a556d4ba7ef400"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24dde96ba13b435e9f0a8a8036348f5f"}},"metadata":{}},{"name":"stderr","text":"The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa56a269a8d4da6a023dab5e0503924"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"688aa7882746498eb90296752fb98722"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa89d63cb66c4d9ea01eb68b63b523c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bfcbebe486a46e6985cb7c4bddcee49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4dcf5779fcc4a6f806f93a622689e51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23ec3d3cba054a3b96111d02e866ee54"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# 4. Example query\nquery = \"Evaluation results on MATH, GSM8K, and MGSM\"\nprint(\"\\nProcessing query:\", query)\n\nresults = query_tables(vector_store, query, deepseek_pipe)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T16:13:31.535558Z","iopub.execute_input":"2025-01-24T16:13:31.535938Z","iopub.status.idle":"2025-01-24T16:14:03.074374Z","shell.execute_reply.started":"2025-01-24T16:13:31.535908Z","shell.execute_reply":"2025-01-24T16:14:03.073667Z"}},"outputs":[{"name":"stdout","text":"\nProcessing query: Evaluation results on MATH, GSM8K, and MGSM\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"print(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T16:14:03.075286Z","iopub.execute_input":"2025-01-24T16:14:03.075492Z","iopub.status.idle":"2025-01-24T16:14:03.080052Z","shell.execute_reply.started":"2025-01-24T16:14:03.075473Z","shell.execute_reply":"2025-01-24T16:14:03.079171Z"}},"outputs":[{"name":"stdout","text":"{'answer': 'Okay, so I need to analyze some PDF table extracts related to LLM evaluations and then answer a specific query about evaluation results on three datasets: MATH, GSM8K, and MGSM. Let me start by understanding what each table contains.\\n\\nFirst, looking at Page 84, Table 10 has two columns listing features of an LLM product and their impacts on real people. The first column mentions things like \"Task alignment is good\" and \"Zero-shot anticipates potential translation capabilities.\" The second column provides scores like \"3 great,\" \"2 good,\" etc., along with notes like \"Can be visualized directly during training.\"\\n\\nThen there\\'s another section in Table 10 that talks about automated scoring, mentioning things like \"Constructed from real world datasets\" and how they\\'re used for evaluating models. It also discusses whether these methods can be compared across different models and their limitations.\\n\\nMoving to Page 84, Table 8 seems similar but focuses more on aspects like \"Multilingual\" support and \"Mitigations\" such as filters and sample ranking. This table emphasizes the ability to approximate real usage and improve model interpretability through documentation and tooling.\\n\\nNow, Page 83, Table 9 includes information about LLM usage by product developers, impact on real people, and aspects like \"No task prompting\" which rely on external APIs. There\\'s also mention of a summary metric for toxicity, indicating this dataset is specifically focused on that aspect.\\n\\nLastly, Page 83, Table 8 again covers similar ground but adds details about mitigating factors like filters and sample ranking to ensure real-world applicability and model interpretability.\\n\\nThe query asks about evaluation results specifically on MATH, GSM8K, and MGSM datasets. From the tables provided, I notice that Tables 8 and 9 on Page 83 discuss aspects related to multilingual populations and mitigation techniques, which could relate to these datasets. However, none of the tables explicitly mention these datasets by name.\\n\\nBut wait, maybe the tables are part of a larger study where these datasets are used implicitly. For example, Table 8 mentions \"multilingual, particularly low-resource languages,\" which could align with MGSM if it\\'s a multilingual dataset. Similarly, GSM8K might be a dataset focusing on something like knowledge graphs or specific tasks like toxicity classification, given that Table 9 discusses toxicity metrics.\\n\\nSo, putting this together:\\n\\n- MATH likely stands for a mathematics-related dataset, perhaps used for tasks like theorem proving or numerical reasoning.\\n- GSM8K might', 'references': [{'page': 84, 'table_num': 10, 'content': '| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\\n| developers within ~3 years.   | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\\n|                               |                                      |                                   | systems.             ...'}, {'page': 84, 'table_num': 10, 'content': '| zero-shot anticipates         | Constructed from real                | into English.                     | Automated scoring                    | Can be visualized directly         |\\n| potential translation         | world datasets, aligned with         | Automated scoring allows          | methods and signals are              | during training.                   |\\n| capabilities of future LLMs.  | harms in real systems.               | using different eval set          | reliable and determin...'}, {'page': 83, 'table_num': 9, 'content': '| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\\n| developers within ~3 years.   | signals are separately validated.    | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\\n|                               |                                      |                                   | systems.             ...'}, {'page': 83, 'table_num': 8, 'content': '| LLM might be used by product    | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\\n| developers within ~3 years.     | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\\n|                                 |                                      |                                   | systems.       ...'}, {'page': 83, 'table_num': 8, 'content': '| With potential mitigations:     | Approximate real usage or            | development and change            | model checkpoints.                   | Documentation and tooling          |\\n| eg, filters, sample and rank    | input distributions (eg, not         | over time.                        |                                      | for interpreting results (eg,      |\\n|                                 | only artificially controlled         |                                   |                ...'}]}\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"print(\"\\nFinal Answer:\")\nprint(results[\"answer\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T16:14:31.270628Z","iopub.execute_input":"2025-01-24T16:14:31.270975Z","iopub.status.idle":"2025-01-24T16:14:31.27591Z","shell.execute_reply.started":"2025-01-24T16:14:31.270946Z","shell.execute_reply":"2025-01-24T16:14:31.275124Z"}},"outputs":[{"name":"stdout","text":"\nFinal Answer:\nOkay, so I need to analyze some PDF table extracts related to LLM evaluations and then answer a specific query about evaluation results on three datasets: MATH, GSM8K, and MGSM. Let me start by understanding what each table contains.\n\nFirst, looking at Page 84, Table 10 has two columns listing features of an LLM product and their impacts on real people. The first column mentions things like \"Task alignment is good\" and \"Zero-shot anticipates potential translation capabilities.\" The second column provides scores like \"3 great,\" \"2 good,\" etc., along with notes like \"Can be visualized directly during training.\"\n\nThen there's another section in Table 10 that talks about automated scoring, mentioning things like \"Constructed from real world datasets\" and how they're used for evaluating models. It also discusses whether these methods can be compared across different models and their limitations.\n\nMoving to Page 84, Table 8 seems similar but focuses more on aspects like \"Multilingual\" support and \"Mitigations\" such as filters and sample ranking. This table emphasizes the ability to approximate real usage and improve model interpretability through documentation and tooling.\n\nNow, Page 83, Table 9 includes information about LLM usage by product developers, impact on real people, and aspects like \"No task prompting\" which rely on external APIs. There's also mention of a summary metric for toxicity, indicating this dataset is specifically focused on that aspect.\n\nLastly, Page 83, Table 8 again covers similar ground but adds details about mitigating factors like filters and sample ranking to ensure real-world applicability and model interpretability.\n\nThe query asks about evaluation results specifically on MATH, GSM8K, and MGSM datasets. From the tables provided, I notice that Tables 8 and 9 on Page 83 discuss aspects related to multilingual populations and mitigation techniques, which could relate to these datasets. However, none of the tables explicitly mention these datasets by name.\n\nBut wait, maybe the tables are part of a larger study where these datasets are used implicitly. For example, Table 8 mentions \"multilingual, particularly low-resource languages,\" which could align with MGSM if it's a multilingual dataset. Similarly, GSM8K might be a dataset focusing on something like knowledge graphs or specific tasks like toxicity classification, given that Table 9 discusses toxicity metrics.\n\nSo, putting this together:\n\n- MATH likely stands for a mathematics-related dataset, perhaps used for tasks like theorem proving or numerical reasoning.\n- GSM8K might\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(\"\\nReference Tables:\")\nfor ref in results[\"references\"]:\n    print(f\"\\nPage {ref['page']} | Table {ref['table_num']}\")\n    print(ref[\"content\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T16:14:45.809631Z","iopub.execute_input":"2025-01-24T16:14:45.810067Z","iopub.status.idle":"2025-01-24T16:14:45.817448Z","shell.execute_reply.started":"2025-01-24T16:14:45.810021Z","shell.execute_reply":"2025-01-24T16:14:45.814971Z"}},"outputs":[{"name":"stdout","text":"\nReference Tables:\n\nPage 84 | Table 10\n| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.   | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                               |                                      |                                   | systems.             ...\n\nPage 84 | Table 10\n| zero-shot anticipates         | Constructed from real                | into English.                     | Automated scoring                    | Can be visualized directly         |\n| potential translation         | world datasets, aligned with         | Automated scoring allows          | methods and signals are              | during training.                   |\n| capabilities of future LLMs.  | harms in real systems.               | using different eval set          | reliable and determin...\n\nPage 83 | Table 9\n| LLM might be used by product  | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.   | signals are separately validated.    | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                               |                                      |                                   | systems.             ...\n\nPage 83 | Table 8\n| LLM might be used by product    | impact on real people.  Scoring and  | cultural diversity in global      | scoring, signals and other noise is  | Comparable across models. Can      |\n| developers within ~3 years.     | signals are  separately validated.   | population and downstream users.  | bound.  Works across models or       | be used to evaluate data or model  |\n|                                 |                                      |                                   | systems.       ...\n\nPage 83 | Table 8\n| With potential mitigations:     | Approximate real usage or            | development and change            | model checkpoints.                   | Documentation and tooling          |\n| eg, filters, sample and rank    | input distributions (eg, not         | over time.                        |                                      | for interpreting results (eg,      |\n|                                 | only artificially controlled         |                                   |                ...\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}