{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/resume-ai-langchainrag?scriptVersionId=226845999\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Install required packages (run this cell if not already installed)\n!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl datasets pypdf langchain-community langchain-huggingface ragatouille","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:56:56.287397Z","iopub.execute_input":"2025-03-10T15:56:56.287707Z","iopub.status.idle":"2025-03-10T15:57:17.925829Z","shell.execute_reply.started":"2025-03-10T15:56:56.287669Z","shell.execute_reply":"2025-03-10T15:57:17.924806Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.2/294.2 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.7/261.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.5/474.5 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    pipeline,\n)\nfrom langchain.document_loaders import PyPDFLoader, DirectoryLoader\nfrom langchain.docstore.document import Document as LangchainDocument\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores.utils import DistanceStrategy\nfrom ragatouille import RAGPretrainedModel\nfrom typing import Optional, List, Tuple\nimport pandas as pd\n\n##########################\n# 1. Load Resume Dataset\n##########################\n\n# Path to the data directory containing resume PDFs\n# Assuming the following structure:\n# data/\n#  ├── HR/\n#  │   ├── resume1.pdf\n#  │   └── resume2.pdf\n#  ├── Designer/\n#  │   ├── resume3.pdf\n#  │   └── resume4.pdf\n#  └── ...\nDATA_DIR = \"/kaggle/input/resume-dataset/data/data\"\nCSV_PATH = \"/kaggle/input/resume-dataset/Resume/Resume.csv\"  # Path to the CSV file with resume metadata\n\n# Load CSV data for metadata\ntry:\n    resume_df = pd.read_csv(CSV_PATH)\n    print(f\"Loaded metadata for {len(resume_df)} resumes\")\nexcept Exception as e:\n    print(f\"Warning: Could not load CSV file. Will proceed without metadata: {e}\")\n    resume_df = None\n\n# List to store all loaded documents\nall_documents = []\n\n# Check if the data directory exists\nif not os.path.exists(DATA_DIR):\n    print(f\"Warning: Data directory '{DATA_DIR}' not found. Please check the path.\")\nelse:\n    # Get all categories (subdirectories)\n    categories = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n    \n    for category in categories:\n        category_path = os.path.join(DATA_DIR, category)\n        print(f\"Loading resumes from category: {category}\")\n        \n        # Use DirectoryLoader to load all PDFs in the category directory\n        loader = DirectoryLoader(\n            category_path, \n            glob=\"**/*.pdf\",  # Load all PDFs, including in subdirectories\n            loader_cls=PyPDFLoader\n        )\n        \n        try:\n            docs = loader.load()\n            # Add metadata: include the category and filename\n            for doc in docs:\n                doc.metadata[\"category\"] = category\n                filename = os.path.basename(doc.metadata[\"source\"])\n                doc.metadata[\"file_name\"] = filename\n                doc.metadata[\"id\"] = os.path.splitext(filename)[0]  # Remove extension to get ID\n                \n                # Add additional metadata from CSV if available\n                if resume_df is not None:\n                    resume_id = doc.metadata[\"id\"]\n                    resume_info = resume_df[resume_df[\"ID\"] == resume_id]\n                    if not resume_info.empty:\n                        # Add any additional metadata from the CSV\n                        pass\n            \n            all_documents.extend(docs)\n            print(f\"  Loaded {len(docs)} resumes from {category}\")\n        except Exception as e:\n            print(f\"  Error loading documents from {category}: {e}\")\n\nprint(f\"Total resumes loaded: {len(all_documents)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:00:24.238739Z","iopub.execute_input":"2025-03-10T16:00:24.239034Z","iopub.status.idle":"2025-03-10T16:11:48.207983Z","shell.execute_reply.started":"2025-03-10T16:00:24.239012Z","shell.execute_reply":"2025-03-10T16:11:48.207282Z"}},"outputs":[{"name":"stdout","text":"Loaded metadata for 2484 resumes\nLoading resumes from category: DESIGNER\n  Loaded 202 resumes from DESIGNER\nLoading resumes from category: BPO\n  Loaded 47 resumes from BPO\nLoading resumes from category: FINANCE\n  Loaded 238 resumes from FINANCE\nLoading resumes from category: CONSTRUCTION\n  Loaded 228 resumes from CONSTRUCTION\nLoading resumes from category: SALES\n  Loaded 205 resumes from SALES\nLoading resumes from category: AUTOMOBILE\n  Loaded 72 resumes from AUTOMOBILE\nLoading resumes from category: CONSULTANT\n  Loaded 236 resumes from CONSULTANT\nLoading resumes from category: CHEF\n  Loaded 230 resumes from CHEF\nLoading resumes from category: APPAREL\n  Loaded 188 resumes from APPAREL\nLoading resumes from category: AGRICULTURE\n  Loaded 132 resumes from AGRICULTURE\nLoading resumes from category: TEACHER\n  Loaded 185 resumes from TEACHER\nLoading resumes from category: HR\n  Loaded 225 resumes from HR\nLoading resumes from category: DIGITAL-MEDIA\n  Loaded 180 resumes from DIGITAL-MEDIA\nLoading resumes from category: ACCOUNTANT\n  Loaded 241 resumes from ACCOUNTANT\nLoading resumes from category: HEALTHCARE\n  Loaded 238 resumes from HEALTHCARE\nLoading resumes from category: INFORMATION-TECHNOLOGY\n  Loaded 247 resumes from INFORMATION-TECHNOLOGY\nLoading resumes from category: ADVOCATE\n  Loaded 244 resumes from ADVOCATE\nLoading resumes from category: FITNESS\n  Loaded 216 resumes from FITNESS\nLoading resumes from category: AVIATION\n  Loaded 219 resumes from AVIATION\nLoading resumes from category: PUBLIC-RELATIONS\n  Loaded 237 resumes from PUBLIC-RELATIONS\nLoading resumes from category: ENGINEERING\n  Loaded 227 resumes from ENGINEERING\nLoading resumes from category: BUSINESS-DEVELOPMENT\n  Loaded 229 resumes from BUSINESS-DEVELOPMENT\nLoading resumes from category: BANKING\n  Loaded 216 resumes from BANKING\nLoading resumes from category: ARTS\n  Loaded 199 resumes from ARTS\nTotal resumes loaded: 4881\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"##########################\n# 2. Split Documents into Chunks\n##########################\n\n# Define the embedding model name (also used for tokenization)\nEMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n\n# Define a list of Markdown separators (from LangChain's MarkdownTextSplitter)\nMARKDOWN_SEPARATORS = [\n    \"\\n#{1,6} \",\n    \"```\\n\",\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n\ndef split_documents(chunk_size: int, knowledge_base: list, tokenizer_name: str = EMBEDDING_MODEL_NAME):\n    \"\"\"\n    Splits documents into chunks using a Hugging Face tokenizer.\n    Adds overlap and uses custom Markdown separators.\n    Removes duplicate chunks.\n    \"\"\"\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n        tokenizer,\n        chunk_size=chunk_size,\n        chunk_overlap=int(chunk_size / 10),\n        add_start_index=True,\n        strip_whitespace=True,\n        separators=MARKDOWN_SEPARATORS,\n    )\n    \n    docs_processed = []\n    for doc in knowledge_base:\n        docs_processed.extend(text_splitter.split_documents([doc]))\n    \n    # Remove duplicates based on content\n    unique_texts = {}\n    docs_processed_unique = []\n    for doc in docs_processed:\n        if doc.page_content not in unique_texts:\n            unique_texts[doc.page_content] = True\n            docs_processed_unique.append(doc)\n    return docs_processed_unique\n\n# Split the loaded documents into chunks (adjust chunk_size as needed)\ndocs_processed = split_documents(512, all_documents, tokenizer_name=EMBEDDING_MODEL_NAME)\nprint(f\"Total chunks after splitting: {len(docs_processed)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:11:48.209008Z","iopub.execute_input":"2025-03-10T16:11:48.209332Z","iopub.status.idle":"2025-03-10T16:12:52.966315Z","shell.execute_reply.started":"2025-03-10T16:11:48.209306Z","shell.execute_reply":"2025-03-10T16:12:52.96554Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/394 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"726647766c6a450eb3bdefaedaa4b44a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a6f7a480304d319575a5eacd343d6a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86af3d53d275463ca04a1b83bc6a63e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c1cb5faedc9464ca0d2d930ed52c90b"}},"metadata":{}},{"name":"stdout","text":"Total chunks after splitting: 11255\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"##########################\n# 3. Build a FAISS Vector Index\n##########################\n\n# Initialize HuggingFaceEmbeddings (using GPU if available)\nembedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"} if torch.cuda.is_available() else {\"device\": \"cpu\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Use cosine similarity\n)\n\n# Create FAISS vector store from the processed document chunks\nKNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:12:52.967727Z","iopub.execute_input":"2025-03-10T16:12:52.96795Z","iopub.status.idle":"2025-03-10T16:13:47.386626Z","shell.execute_reply.started":"2025-03-10T16:12:52.967931Z","shell.execute_reply":"2025-03-10T16:13:47.385847Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f06890898c784fc88f0c5be0a8a212d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf38017b9d6142f7a794d2160614af16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7bc51492f5a494ba92390d4506a821f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7acaacc3b2f4a5e9c6718d5c6c62140"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/66.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a2d6775011347feb0efb53914930bac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3890c99a0998487cb02db09adf3b87b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4565c2dc9e8a4514ac10669ba4c3e410"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"##########################\n# 4. Set Up the Open-Source LLM for Answer Generation\n##########################\n\nREADER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-beta\"\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(READER_MODEL_NAME, quantization_config=bnb_config)\ntokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n\nREADER_LLM = pipeline(\n    model=model,\n    tokenizer=tokenizer,\n    task=\"text-generation\",\n    do_sample=True,\n    temperature=0.2,\n    repetition_penalty=1.1,\n    return_full_text=False,\n    max_new_tokens=500,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:13:47.387909Z","iopub.execute_input":"2025-03-10T16:13:47.388212Z","iopub.status.idle":"2025-03-10T16:15:48.957153Z","shell.execute_reply.started":"2025-03-10T16:13:47.388178Z","shell.execute_reply":"2025-03-10T16:15:48.956463Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a54bb50bcc74275a3c7c47cc7437801"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f52d1e61f7a42a3aa50f10da4ff67c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa7aa18bfba74c84a45abf7c3ae5d971"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ece2781acef4af38597d4c92dd92ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e19f22264c4bc787d96b24e920efa1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd99cacde39f4637b06e14ad5ba30daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdb9619130e4af6908f0f8b48f900eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"618b77f3a8ee4d02bebbb42aca01963a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d592b46130241aaa19d67f1a47f3bdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f93014fb8ec455aa95f6e6a51bac0ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9192a7bff6b54e45908c34a6958fa216"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee04d8749474cd3b272f485279f8e92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95b185bd46f74b70ae17dfd854ef8bb9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41d725dd79ae426895de1b6264c6b1c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1aad09b992624b0d867698c9ea174181"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45712cbded634ca5af0688194d784ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce392743adbb44a09918414343c65477"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"713eb43e11f84467b7f8ea0cf0b4e989"}},"metadata":{}},{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"##########################\n# 5. Define the Chat-Style Prompt Template for Resume Analysis\n##########################\n\nprompt_in_chat_format = [\n    {\n        \"role\": \"system\",\n        \"content\": (\n            \"You are a resume analysis assistant. Using only the information contained in the provided resume segments, \"\n            \"answer the user's question accurately and concisely.\\n\\n\"\n            \"- Strictly refer to the provided context—do not generate or assume information.\\n\"\n            \"- Include the source document number and category when applicable.\\n\"\n            \"- If the context does not provide enough information, explicitly state: 'The answer cannot be determined from the provided resume segments.'\\n\"\n            \"- Ensure neutrality and objectivity in your analysis.\"\n        ),\n    },\n    {\n        \"role\": \"user\",\n        \"content\": (\n            \"Resume segments:\\n{context}\\n---\\nNow, answer the following question based on the provided resume segments.\\n\\n\"\n            \"Question: {question}\"\n        ),\n    },\n]\n\n# Use the tokenizer's helper to apply the chat template\nRAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n)\n\n##########################\n# 6. Initialize the Reranker\n##########################\n\n# Load a reranker model (e.g., from ColBERT v2)\nRERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:15:48.957982Z","iopub.execute_input":"2025-03-10T16:15:48.958202Z","iopub.status.idle":"2025-03-10T16:15:53.019689Z","shell.execute_reply.started":"2025-03-10T16:15:48.958174Z","shell.execute_reply":"2025-03-10T16:15:53.018803Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a258fea03c34b529f0be5deb893efcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9392e4bedb664ed78b2e3f19c65271eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ababb915785e48dda91c256d6b7a28e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bc9a5bd9b5b46988df883fb96bc7bc3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16d9c050b7ba4d99bbd6eb727a077344"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6b7256b9f76490090a64935e993e9c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851e7ecdd61b4bbb8efbfc34df79c760"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler()\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"##########################\n# 7. Define the RAG Function with Re-Ranking for Resume Analysis\n##########################\n\ndef answer_with_rag(\n    question: str,\n    llm: pipeline,\n    knowledge_index: FAISS,\n    reranker: Optional[RAGPretrainedModel] = None,\n    num_retrieved_docs: int = 30,\n    num_docs_final: int = 5,\n    category_filter: Optional[str] = None,\n) -> Tuple[str, List[str], List[dict]]:\n    print(\"=> Retrieving resume segments...\")\n    \n    # Retrieve a larger set of candidate documents\n    # If a category filter is provided, we'll filter results post-retrieval\n    retrieved_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n    \n    # Apply category filter if specified\n    if category_filter:\n        retrieved_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"category\") == category_filter]\n        # If filtering reduced docs below our final number, adjust\n        num_docs_final = min(num_docs_final, len(retrieved_docs))\n    \n    retrieved_texts = [doc.page_content for doc in retrieved_docs]\n    retrieved_metadata = [doc.metadata for doc in retrieved_docs]\n    \n    # Optionally rerank the retrieved documents\n    if reranker and len(retrieved_docs) > num_docs_final:\n        print(\"=> Reranking resume segments...\")\n        reranked = reranker.rerank(question, retrieved_texts, k=num_docs_final)\n        \n        # Reranker returns a list of dicts with \"content\" and indices\n        final_texts = [item[\"content\"] for item in reranked]\n        \n        # Get the corresponding metadata for reranked documents\n        final_metadata = []\n        for item in reranked:\n            if \"index\" in item:\n                final_metadata.append(retrieved_metadata[item[\"index\"]])\n            else:\n                # If index not available, try to match by content\n                idx = retrieved_texts.index(item[\"content\"]) if item[\"content\"] in retrieved_texts else None\n                if idx is not None:\n                    final_metadata.append(retrieved_metadata[idx])\n                else:\n                    # Fallback if we can't match\n                    final_metadata.append({})\n    else:\n        final_texts = retrieved_texts[:num_docs_final]\n        final_metadata = retrieved_metadata[:num_docs_final]\n    \n    # Build the final context string from the top documents\n    context = \"\\nResume segments:\\n\"\n    for i, (text, metadata) in enumerate(zip(final_texts, final_metadata)):\n        category = metadata.get(\"category\", \"Unknown\")\n        filename = metadata.get(\"file_name\", \"Unknown\")\n        context += f\"Document {i} (Category: {category}, File: {filename}):::\\n{text}\\n\\n\"\n    \n    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n    print(\"=> Generating answer...\")\n    answer = llm(final_prompt)[0][\"generated_text\"]\n    print(\"Answer Generated!!!\")\n    return answer, final_texts, final_metadata\n\n##########################\n# 8. Query Function with Optional Category Filter\n##########################\n\ndef query_resumes(user_query, category=None):\n    \"\"\"\n    Query the resume database with an option to filter by job category.\n    \n    Args:\n        user_query (str): User's question about resumes\n        category (str, optional): Category to filter by (e.g., \"HR\", \"Designer\")\n        \n    Returns:\n        dict: Contains the answer and metadata about retrieved documents\n    \"\"\"\n    # Generate the answer using the RAG function with re-ranking\n    answer, final_docs, final_metadata = answer_with_rag(\n        user_query, \n        READER_LLM, \n        KNOWLEDGE_VECTOR_DATABASE, \n        reranker=RERANKER,\n        category_filter=category\n    )\n    \n    # Format results\n    result = {\n        \"answer\": answer,\n        \"retrieved_docs\": []\n    }\n    \n    # Add retrieved document info\n    for i, (doc, metadata) in enumerate(zip(final_docs, final_metadata)):\n        doc_info = {\n            \"index\": i,\n            \"category\": metadata.get(\"category\", \"Unknown\"),\n            \"file_name\": metadata.get(\"file_name\", \"Unknown\"),\n            \"id\": metadata.get(\"id\", \"Unknown\"),\n            \"content_preview\": doc[:200] + \"...\" if len(doc) > 200 else doc\n        }\n        result[\"retrieved_docs\"].append(doc_info)\n        \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:15:53.020474Z","iopub.execute_input":"2025-03-10T16:15:53.020671Z","iopub.status.idle":"2025-03-10T16:15:53.031049Z","shell.execute_reply.started":"2025-03-10T16:15:53.020654Z","shell.execute_reply":"2025-03-10T16:15:53.030087Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"##########################\n# 9. Sample Usage\n##########################\n\n# Example queries:\nsample_queries = [\n    \"What skills are common in IT resumes?\",\n    \"Compare the education backgrounds in HR vs Finance resumes\",\n    \"What certifications are popular in Healthcare resumes?\",\n    \"Find resumes with experience in project management\",\n    \"What are common job titles in Business Development resumes?\"\n]\n\n# Example usage:\nif len(all_documents) > 0:\n    print(\"\\n==================================Sample Query==================================\\n\")\n    # Example: Query with category filter\n    sample_query = \"What skills are commonly mentioned in these resumes?\"\n    category_filter = None  # Set to a specific category like \"Information-Technology\" or None for all\n    \n    result = query_resumes(sample_query, category=category_filter)\n    \n    print(f\"Question: {sample_query}\")\n    if category_filter:\n        print(f\"Category Filter: {category_filter}\")\n    \n    print(\"\\n==================================Answer==================================\\n\")\n    print(result[\"answer\"])\n    \n    print(\"\\n==================================Retrieved Document Info==================================\")\n    for doc in result[\"retrieved_docs\"]:\n        print(f\"\\nDocument {doc['index']}:\")\n        print(f\"  Category: {doc['category']}\")\n        print(f\"  File: {doc['file_name']}\")\n        print(f\"  ID: {doc['id']}\")\n        print(f\"  Preview: {doc['conte nt_preview']}\")\nelse:\n    print(\"No documents were loaded. Please check the data directory path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:15:53.03188Z","iopub.execute_input":"2025-03-10T16:15:53.032082Z","iopub.status.idle":"2025-03-10T16:16:19.600574Z","shell.execute_reply.started":"2025-03-10T16:15:53.032064Z","shell.execute_reply":"2025-03-10T16:16:19.599634Z"}},"outputs":[{"name":"stdout","text":"\n==================================Sample Query==================================\n\n=> Retrieving resume segments...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ade7763708dd444bacc28fb78e03719a"}},"metadata":{}},{"name":"stdout","text":"=> Reranking resume segments...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/colbert/utils/amp.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  return torch.cuda.amp.autocast() if self.activated else NullContextManager()\n100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\nAnswer Generated!!!\nQuestion: What skills are commonly mentioned in these resumes?\n\n==================================Answer==================================\n\nSome commonly mentioned skills across all the resumes include:\n\n1. Multi-tasking\n2. Communication skills (verbal and written)\n3. Problem solving\n4. Time management\n5. Leadership\n6. Customer service\n7. Organization skills\n8. Microsoft Office proficiency (specifically Excel, PowerPoint, and Word)\n9. Email and phone skills\n10. Team building\n11. Sales experience\n12. Ability to work well with others\n13. Project management skills\n14. Problem identification and resolution\n15. Attention to detail\n16. Familiarity with accounting systems and principles\n17. Familiarity with aviation policies and procedures\n18. Familiarity with fitness promotion and goal setting\n19. Familiarity with HR practices such as employee relations and handbook creation\n\nNote that this list is not exhaustive and may vary depending on the specific job or industry being applied for.\n\n==================================Retrieved Document Info==================================\n\nDocument 0:\n  Category: CHEF\n  File: 36366044.pdf\n  ID: 36366044\n  Preview: Skills\nOther Skills Include: Public Speaking, Leadership, Team Building, Establishing Good Rapport, Time Management, Self Motivation,\nProblem Solving, Ability To Analyze Sales As Well As Guest Satisfa...\n\nDocument 1:\n  Category: AVIATION\n  File: 12504278.pdf\n  ID: 12504278\n  Preview: Skills\naccounting, accounting system, customer service, inventory, listening, managerial, managing, multi-tasking, organizational skills, policies, proposals,\nquality, Research, strategy, supervising,...\n\nDocument 2:\n  Category: CONSULTANT\n  File: 18856440.pdf\n  ID: 18856440\n  Preview: General\nSkills\nCash handling, Shipping and receiving, Professional and friendly, Careful and active listener,Multi-tasking, 10-Key, Active Learning, Calendaring,\nCustomer Needs, Customer Service, Data...\n\nDocument 3:\n  Category: FITNESS\n  File: 12019284.pdf\n  ID: 12019284\n  Preview: Skills\nbilling, blood pressure, brochures, communication skills, clients, email, goal setting, promote health, leadership, Director, marketing, market,\nmarketing materials, materials, Microsoft Office...\n\nDocument 4:\n  Category: HR\n  File: 30163002.pdf\n  ID: 30163002\n  Preview: Skills\nbusiness development, central point of contact, closing, client, clients, email, employee relations, fashion, fast, faxes, meetings, access, mail, office,\nPowerPoint presentations, neat, payrol...\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"##########################\n# 10. Save the Vector Store for Future Use\n##########################\n\n# Save the FAISS index\nKNOWLEDGE_VECTOR_DATABASE.save_local(\"resume_vector_store\")\nprint(\"\\nSaved vector store to 'resume_vector_store'. You can load it later with:\")\nprint(\"from langchain.vectorstores import FAISS\")\nprint(\"from langchain_huggingface import HuggingFaceEmbeddings\")\nprint(\"embedding_model = HuggingFaceEmbeddings(model_name='thenlper/gte-small')\")\nprint(\"vector_store = FAISS.load_local('resume_vector_store', embedding_model)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:48:35.434985Z","iopub.execute_input":"2025-03-10T16:48:35.435337Z","iopub.status.idle":"2025-03-10T16:48:35.548088Z","shell.execute_reply.started":"2025-03-10T16:48:35.435309Z","shell.execute_reply":"2025-03-10T16:48:35.547129Z"}},"outputs":[{"name":"stdout","text":"\nSaved vector store to 'resume_vector_store'. You can load it later with:\nfrom langchain.vectorstores import FAISS\nfrom langchain_huggingface import HuggingFaceEmbeddings\nembedding_model = HuggingFaceEmbeddings(model_name='thenlper/gte-small')\nvector_store = FAISS.load_local('resume_vector_store', embedding_model)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"sample_query = \"\"\"\nThe following is the job description: \n'Job Title: Business Development Consultant\n\nJob Duties:\n\nDeveloping connections and new sources of business and relationship management with existing accounts\nNetworking and building relationships with business partners and jobseekers\nIdentifying and pursuing new business opportunities to expand the client base\nManaging key accounts to ensure the delivery standard and fulfilment level\nProvide financial analysis and modeling to support business cases.\nBusiness partnering with internal stakeholders to improve current processes and implement new procedures\nConducting market research and competitive analysis to identify trends and opportunities for growth\nConsult with clients to understand their needs and provide tailored solutions.\nPreparing and delivering presentations and proposals to potential clients\nDeveloping strategies for brand promotion and lead generation\nOther ad-hoc duties apply\n\n\nRequirements:\n\nMinimum Diploma / Degree in Business Administration of Management\nAt least 2 year of Sales / Business development experience\nGood communication and interpersonal skills\nExcellent time management and organizational skills\nThe potential candidate must be motivated to stay ahead by keeping up with the latest technological advancements and market trends.'\n\nYour tasks are as follows:\n\n1. Analyze all the resumes to assess its alignment with the provided job description.\n2. Stick strictly to the facts provided in the resumes and job description. Never make assumptions or invent information.\n3. Select only the top 3 resumes based on their alignment with the job description. Ensure these are ranked clearly by relevance (1st: Most Suitable, 2nd: Next Best, 3rd: Final Option).\n4. Evaluate key areas, including:\n    Education: Does the candidate meet the educational requirements?\n    Experience: Is their work experience relevant and sufficient for the role?\n    Skills: Are the technical, soft, and domain-specific skills present?\n5. For each resume, provide:\n    Strengths: Key qualifications that match the role.\n    Gaps: Areas where the candidate does not meet the criteria.\n6. Summarize why these top 3 resumes were chosen, focusing on their alignment with the job requirements.\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:44:58.288017Z","iopub.execute_input":"2025-03-10T16:44:58.288372Z","iopub.status.idle":"2025-03-10T16:44:58.292715Z","shell.execute_reply.started":"2025-03-10T16:44:58.288345Z","shell.execute_reply":"2025-03-10T16:44:58.291739Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"\n# Example usage:\nif len(all_documents) > 0:\n    print(\"\\n==================================Sample Query==================================\\n\")\n    # Example: Query with category filter\n    # sample_query = \"What skills are commonly mentioned in these resumes?\"\n    category_filter = None  # Set to a specific category like \"Information-Technology\" or None for all\n    \n    result = query_resumes(sample_query, category=category_filter)\n    \n    print(f\"Question: {sample_query}\")\n    if category_filter:\n        print(f\"Category Filter: {category_filter}\")\n    \n    print(\"\\n==================================Answer==================================\\n\")\n    print(result[\"answer\"])\n    \n    print(\"\\n==================================Retrieved Document Info==================================\")\n    for doc in result[\"retrieved_docs\"]:\n        print(f\"\\nDocument {doc['index']}:\")\n        print(f\"  Category: {doc['category']}\")\n        print(f\"  File: {doc['file_name']}\")\n        print(f\"  ID: {doc['id']}\")\n        print(f\"  Preview: {doc['content_preview']}\")\nelse:\n    print(\"No documents were loaded. Please check the data directory path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T16:44:58.977828Z","iopub.execute_input":"2025-03-10T16:44:58.978109Z","iopub.status.idle":"2025-03-10T16:46:07.11635Z","shell.execute_reply.started":"2025-03-10T16:44:58.978086Z","shell.execute_reply":"2025-03-10T16:46:07.115654Z"}},"outputs":[{"name":"stdout","text":"\n==================================Sample Query==================================\n\n=> Retrieving resume segments...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18acfc80fe04b779c0946047ef2d5d6"}},"metadata":{}},{"name":"stdout","text":"=> Reranking resume segments...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  2.66it/s]\n","output_type":"stream"},{"name":"stdout","text":"=> Generating answer...\nAnswer Generated!!!\nQuestion: \nThe following is the job description: \n'Job Title: Business Development Consultant\n\nJob Duties:\n\nDeveloping connections and new sources of business and relationship management with existing accounts\nNetworking and building relationships with business partners and jobseekers\nIdentifying and pursuing new business opportunities to expand the client base\nManaging key accounts to ensure the delivery standard and fulfilment level\nProvide financial analysis and modeling to support business cases.\nBusiness partnering with internal stakeholders to improve current processes and implement new procedures\nConducting market research and competitive analysis to identify trends and opportunities for growth\nConsult with clients to understand their needs and provide tailored solutions.\nPreparing and delivering presentations and proposals to potential clients\nDeveloping strategies for brand promotion and lead generation\nOther ad-hoc duties apply\n\n\nRequirements:\n\nMinimum Diploma / Degree in Business Administration of Management\nAt least 2 year of Sales / Business development experience\nGood communication and interpersonal skills\nExcellent time management and organizational skills\nThe potential candidate must be motivated to stay ahead by keeping up with the latest technological advancements and market trends.'\n\nYour tasks are as follows:\n\n1. Analyze all the resumes to assess its alignment with the provided job description.\n2. Stick strictly to the facts provided in the resumes and job description. Never make assumptions or invent information.\n3. Select only the top 3 resumes based on their alignment with the job description. Ensure these are ranked clearly by relevance (1st: Most Suitable, 2nd: Next Best, 3rd: Final Option).\n4. Evaluate key areas, including:\n    Education: Does the candidate meet the educational requirements?\n    Experience: Is their work experience relevant and sufficient for the role?\n    Skills: Are the technical, soft, and domain-specific skills present?\n5. For each resume, provide:\n    Strengths: Key qualifications that match the role.\n    Gaps: Areas where the candidate does not meet the criteria.\n6. Summarize why these top 3 resumes were chosen, focusing on their alignment with the job requirements.\n\n\n==================================Answer==================================\n\nBased on the provided job description and resume segments, I have analyzed the resumes and selected the top three candidates based on their alignment with the job requirements.\n\n1. Resume Segments: Document 0 (Category: BUSINESS-DEVELOPMENT, File: 18311419.pdf)\n   - Results-oriented Business development professional with a 10-year track record of surpassing sales quotas within highly competitive markets across a broad range of industries.\n   - Exceptional communicator with a consultative sales style, success in leading effective strategies to improve problem-solving abilities, and a keen client needs assessment aptitude.\n   - Aggressively identify opportunities, develop focus, and provide tactical business solutions.\n   - Promote team effectiveness through orientation, on-going training, and performance feedback.\n\n   Strengths: This candidate has a proven track record of achieving sales targets in competitive markets, possesses excellent communication skills, and has experience in developing effective strategies to solve problems and meet client needs. They also have experience promoting team effectiveness through training and feedback.\n\n   Gaps: It is unclear whether this candidate has experience managing key accounts to ensure delivery standards and fulfillment levels, as well as conducting market research and competitive analysis. However, they do mention identifying opportunities and providing tactical business solutions, which could potentially include market research and analysis.\n\n2. Resume Segments: Document 1 (Category: BUSINESS-DEVELOPMENT, File: 12377803.pdf)\n   - Identifies and capitalizing on opportunities.\n   - Understanding customer requirements.\n   - Networking in a professional manner.\n   - Developing key relationships.\n   - Identifying important decision makers.\n   - Maximizing revenue at every opportunity.\n   - Knowledge of how to get past the screening processes and to be able to contact important decision makers.\n\n   Strengths: This candidate has experience identifying opportunities, understanding customer requirements, networking professionally, developing key relationships, identifying important decision makers, maximizing revenue, and navigating screening processes to contact decision makers.\n\n   Gaps: It is unclear whether this candidate has experience managing key accounts to ensure delivery standards and fulfillment levels, as well as conducting market research and\n\n==================================Retrieved Document Info==================================\n\nDocument 0:\n  Category: BUSINESS-DEVELOPMENT\n  File: 18311419.pdf\n  ID: 18311419\n  Preview: STAFFING BUSINESS DEVELOPMENT MANAGER\nSummary\nResults-oriented Business development professional with a 10-year track record of surpassing sales quotas within highly competitive markets\nacross a broad...\n\nDocument 1:\n  Category: BUSINESS-DEVELOPMENT\n  File: 12377803.pdf\n  ID: 12377803\n  Preview: BUSINESS DEVELOPMENT MANAGER\nHighlights\nBUSINESS DEVELOPMENT SKILLS\nIdentify and capitalizing on opportunities.\nUnderstanding customer requirements.\nNetworking in a professional manner.\nDeveloping key...\n\nDocument 2:\n  Category: BUSINESS-DEVELOPMENT\n  File: 39237915.pdf\n  ID: 39237915\n  Preview: BUSINESS DEVELOPMENT MANAGER\nProfessional Profile\nHighly qualified, detail-oriented and hardworking Manager with more than 15 years of experience. Proficient in research, writing, case\nmanagement and ...\n\nDocument 3:\n  Category: BUSINESS-DEVELOPMENT\n  File: 59696315.pdf\n  ID: 59696315\n  Preview: Intelligence gathering on clients, target prospects and industries to inform and support pursuit efforts.\nDevelop ongoing improvements to the process of prospecting, qualifying and closing key account...\n\nDocument 4:\n  Category: BUSINESS-DEVELOPMENT\n  File: 59696315.pdf\n  ID: 59696315\n  Preview: BUSINESS DEVELOPMENT DIRECTOR\nSummary\nI am looking for a challenging Business Development position that will utilize my knowledge and passion for sales while leveraging more than\ntwenty years of relat...\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}