{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/token-to-vector-conversion?scriptVersionId=213821989\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Token-to-Vector Conversion: Transforming Tokens into Numerical Representations\n\nMachine learning models require numerical inputs, so the text must be converted into vectors. This process, known as **Token-to-Vector Conversion**, is critical in NLP pipelines. Let's dive deeper into the various techniques.\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 1. **Bag of Words (BoW)**\n\nBoW represents a text as a sparse vector of word counts or binary indicators for each word in the vocabulary.\n\n#### Process:\n1. Create a vocabulary of unique words from the dataset.\n2. Count the occurrences of each word in a text.\n\n#### Example:\nText: `\"I love NLP. NLP is amazing!\"`\n\n- Vocabulary: `[\"I\", \"love\", \"NLP\", \"is\", \"amazing\"]`\n- BoW Vector for the text: `[1, 1, 2, 1, 1]`\n\n#### Code:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n\ntexts = [\"I love NLP.\", \"NLP is amazing!\"]\nvectorizer = CountVectorizer()\nbow_vectors = vectorizer.fit_transform(texts)\n\nprint(vectorizer.get_feature_names_out())  # Vocabulary\nprint(bow_vectors.toarray())  # Vectors\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:29:42.317651Z","iopub.execute_input":"2024-12-19T08:29:42.318009Z","iopub.status.idle":"2024-12-19T08:29:42.880764Z","shell.execute_reply.started":"2024-12-19T08:29:42.317979Z","shell.execute_reply":"2024-12-19T08:29:42.879813Z"}},"outputs":[{"name":"stdout","text":"['amazing' 'is' 'love' 'nlp']\n[[0 0 1 1]\n [1 1 0 1]]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### Pros:\n- Simple and easy to implement.\n- Works well for small datasets.\n\n#### Cons:\n- Ignores word order (context).\n- High-dimensional and sparse for large vocabularies.\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n\nTF-IDF adjusts word frequency by considering its importance in the entire corpus. Words common across many documents are weighted lower.\n\n#### Formula:\n- **TF**: Term Frequency = (Number of times a word appears in a document) / (Total number of words in the document)\n- **IDF**: Inverse Document Frequency = \\( \\log(\\frac{N}{n}) \\), where \\( N \\) is the total number of documents, and \\( n \\) is the number of documents containing the word.\n\n#### Example:\nText: `\"I love NLP. NLP is amazing!\"`  \nCorpus: `[\"I love NLP.\", \"NLP is amazing!\", \"I enjoy learning NLP.\"]`\n\n- TF-IDF vectors will assign lower weights to frequently occurring words like \"NLP\".\n\n#### Code:","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntexts = [\"I love NLP.\", \"NLP is amazing!\", \"I enjoy learning NLP.\"]\nvectorizer = TfidfVectorizer()\ntfidf_vectors = vectorizer.fit_transform(texts)\n\nprint(vectorizer.get_feature_names_out())  # Vocabulary\nprint(tfidf_vectors.toarray())  # Vectors","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:30:32.092935Z","iopub.execute_input":"2024-12-19T08:30:32.093444Z","iopub.status.idle":"2024-12-19T08:30:32.109488Z","shell.execute_reply.started":"2024-12-19T08:30:32.093408Z","shell.execute_reply":"2024-12-19T08:30:32.10834Z"}},"outputs":[{"name":"stdout","text":"['amazing' 'enjoy' 'is' 'learning' 'love' 'nlp']\n[[0.         0.         0.         0.         0.861037   0.50854232]\n [0.65249088 0.         0.65249088 0.         0.         0.38537163]\n [0.         0.65249088 0.         0.65249088 0.         0.38537163]]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"#### Pros:\n- Highlights important words in context.\n- Reduces the impact of common words.\n\n#### Cons:\n- Still sparse and high-dimensional.\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 3. **Word Embeddings**\n\nWord embeddings are dense vector representations of words that capture semantic meaning. Words with similar meanings are closer in the vector space.\n\n#### Techniques:\n1. **Pre-trained Embeddings**:\n   - **Word2Vec**: Uses Skip-gram or CBOW to learn word relationships.\n   - **GloVe (Global Vectors)**: Captures statistical co-occurrence of words in a corpus.\n   - **FastText**: Handles subword-level embeddings, useful for morphologically rich languages.\n\n2. **Custom Embeddings**:\n   - Train embeddings on your dataset using tools like Gensim or TensorFlow.\n\n#### Example:\n- \"king\" and \"queen\" might have embeddings like:\n  - **king**: `[0.25, 0.35, 0.85, ...]`\n  - **queen**: `[0.23, 0.34, 0.83, ...]`\n\n#### Code (Word2Vec using Gensim):","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from gensim.models import Word2Vec\n\nsentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"amazing\"], [\"I\", \"enjoy\", \"learning\", \"NLP\"]]\nmodel = Word2Vec(sentences, vector_size=10, window=3, min_count=1, workers=4)\n\nprint(model.wv[\"NLP\"])  # Word embedding for \"NLP\"\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:31:41.28084Z","iopub.execute_input":"2024-12-19T08:31:41.281185Z","iopub.status.idle":"2024-12-19T08:31:52.648775Z","shell.execute_reply.started":"2024-12-19T08:31:41.281155Z","shell.execute_reply":"2024-12-19T08:31:52.647795Z"}},"outputs":[{"name":"stdout","text":"[-0.00536227  0.00236431  0.0510335   0.09009273 -0.0930295  -0.07116809\n  0.06458873  0.08972988 -0.05015428 -0.03763372]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#### Pros:\n- Captures semantic relationships.\n- Compact representation.\n\n#### Cons:\n- Requires a large corpus for training.\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### 4. **Contextual Word Embeddings**\n\nThese embeddings generate word vectors **in context**, meaning the same word can have different vectors depending on the sentence.\n\n#### Popular Models:\n- **BERT (Bidirectional Encoder Representations from Transformers)**: Generates dynamic embeddings.\n- **GPT**: Focused on generative tasks.\n\n#### Example:\n- In \"Iâ€™m going to the bank to fish\" vs. \"I deposited money in the bank,\" the word \"bank\" will have different embeddings.\n\n#### Code (Hugging Face Transformers):","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = AutoModel.from_pretrained(\"bert-base-uncased\")\n\ntext = \"NLP is amazing!\"\ntokens = tokenizer(text, return_tensors=\"pt\")\noutput = model(**tokens)\n\nprint(output.last_hidden_state)  # Contextual embeddings\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:32:25.976058Z","iopub.execute_input":"2024-12-19T08:32:25.976674Z","iopub.status.idle":"2024-12-19T08:32:36.271119Z","shell.execute_reply.started":"2024-12-19T08:32:25.97664Z","shell.execute_reply":"2024-12-19T08:32:36.269943Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e979c8ed97364cffb48b5fad64475154"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef24035cc6bb4123a68794e4e141647a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f788c2345e154e2b83a735b0e18b429c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6ed3625cb164c788b4ef8af3ad77457"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"461e8ce22a7f49ab816f79560475a737"}},"metadata":{}},{"name":"stdout","text":"tensor([[[ 0.0762,  0.0177,  0.0297,  ..., -0.2109,  0.2140,  0.3130],\n         [ 0.1237, -1.0247,  0.6718,  ..., -0.3978,  0.9643,  0.7842],\n         [-0.2696, -0.3281,  0.3901,  ..., -0.4611, -0.3425,  0.2534],\n         ...,\n         [ 0.2051,  0.2455, -0.1299,  ..., -0.3859,  0.1110, -0.4565],\n         [-0.0241, -0.7068, -0.4130,  ...,  0.9138,  0.3254, -0.5250],\n         [ 0.7112,  0.0574, -0.2164,  ...,  0.2193, -0.7162, -0.1466]]],\n       grad_fn=<NativeLayerNormBackward0>)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:33:31.440416Z","iopub.execute_input":"2024-12-19T08:33:31.440896Z","iopub.status.idle":"2024-12-19T08:33:31.449478Z","shell.execute_reply.started":"2024-12-19T08:33:31.440853Z","shell.execute_reply":"2024-12-19T08:33:31.448424Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101, 17953,  2361,  2003,  6429,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"output.last_hidden_state.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:33:10.925733Z","iopub.execute_input":"2024-12-19T08:33:10.926366Z","iopub.status.idle":"2024-12-19T08:33:10.932965Z","shell.execute_reply.started":"2024-12-19T08:33:10.926328Z","shell.execute_reply":"2024-12-19T08:33:10.931739Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"torch.Size([1, 7, 768])"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"output.last_hidden_state","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:34:39.667458Z","iopub.execute_input":"2024-12-19T08:34:39.66784Z","iopub.status.idle":"2024-12-19T08:34:39.676239Z","shell.execute_reply.started":"2024-12-19T08:34:39.667809Z","shell.execute_reply":"2024-12-19T08:34:39.675312Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"tensor([[[ 0.0762,  0.0177,  0.0297,  ..., -0.2109,  0.2140,  0.3130],\n         [ 0.1237, -1.0247,  0.6718,  ..., -0.3978,  0.9643,  0.7842],\n         [-0.2696, -0.3281,  0.3901,  ..., -0.4611, -0.3425,  0.2534],\n         ...,\n         [ 0.2051,  0.2455, -0.1299,  ..., -0.3859,  0.1110, -0.4565],\n         [-0.0241, -0.7068, -0.4130,  ...,  0.9138,  0.3254, -0.5250],\n         [ 0.7112,  0.0574, -0.2164,  ...,  0.2193, -0.7162, -0.1466]]],\n       grad_fn=<NativeLayerNormBackward0>)"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"#### Pros:\n- Captures word meaning dynamically based on context.\n- State-of-the-art performance in NLP tasks.\n\n#### Cons:\n- Computationally expensive.\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"### Comparison of Techniques\n\n| Method            | Dimensionality | Context Awareness | Sparsity | Use Cases                                   |\n|--------------------|----------------|--------------------|----------|--------------------------------------------|\n| **Bag of Words**   | High           | No                 | Yes      | Simple text classification                 |\n| **TF-IDF**         | High           | No                 | Yes      | Document ranking, text similarity          |\n| **Word Embeddings**| Low            | Partial            | No       | Semantic analysis, downstream NLP tasks    |\n| **Contextual Embeddings**| Low      | Yes                | No       | Advanced NLP tasks like NER, QA, sentiment |\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}}]}