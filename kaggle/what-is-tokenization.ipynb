{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/what-is-tokenization?scriptVersionId=213819814\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# What is Tokenization?\n\nIn **natural language processing (NLP)**, **tokenization** is the process of breaking down a text into smaller units, called **tokens**. These tokens can be:\n\n- **Words** (e.g., \"I love programming\" → [\"I\", \"love\", \"programming\"])\n- **Subwords** or morphemes (e.g., \"unbelievable\" → [\"un\", \"believe\", \"able\"])\n- **Characters** (e.g., \"hello\" → [\"h\", \"e\", \"l\", \"l\", \"o\"])\n- **Sentences** (splitting text into sentence-level tokens)\n\nTokenization is a crucial preprocessing step for text data because most machine learning models can't process raw text directly—they need a numerical or structured input, which tokenization helps achieve.\n\n---\n\n### Why is Tokenization Important?\n\n1. **Understanding Structure**: It helps the model recognize the components of a text.\n2. **Standardization**: Tokenization standardizes text input for further processing.\n3. **Feature Extraction**: Tokens serve as the features that models use to learn patterns in the data.\n\n---\n\n### Types of Tokenization\n\n#### 1. **Word Tokenization**\n   - Splits text into words.\n   - Example: `\"I love NLP\"` → `[\"I\", \"love\", \"NLP\"]`\n   - Challenges:\n     - Handling contractions like \"don't\" → [\"do\", \"n't\"].\n     - Treating punctuation as separate tokens.\n\n#### 2. **Subword Tokenization**\n   - Breaks words into smaller units when the word is not in the vocabulary.\n   - Common in deep learning models like BERT or GPT.\n   - Example: `\"unbelievable\"` → `[\"un\", \"##believe\", \"##able\"]` (BERT-style)\n   - Benefits:\n     - Reduces the size of the vocabulary.\n     - Handles unseen words effectively.\n\n#### 3. **Character Tokenization**\n   - Breaks text into individual characters.\n   - Example: `\"Hello\"` → `[\"H\", \"e\", \"l\", \"l\", \"o\"]`\n   - Useful for languages with large character sets or when spelling matters.\n\n#### 4. **Sentence Tokenization**\n   - Splits text into sentences.\n   - Example: `\"I love NLP. It’s fascinating.\"` → `[\"I love NLP.\", \"It’s fascinating.\"]`\n   - Often requires handling punctuation correctly.\n\n---\n\n### Methods of Tokenization\n\n#### **Rule-based Tokenization**\n   - Uses predefined rules like splitting on spaces or punctuation.\n   - Simple but struggles with edge cases (e.g., abbreviations, numbers, etc.).\n\n#### **Statistical Tokenization**\n   - Uses probabilities and patterns to decide where to split.\n   - Example: WordPiece and Byte Pair Encoding (BPE).\n\n#### **Neural Tokenization**\n   - Relies on machine learning models to learn how to tokenize.\n   - Example: SentencePiece, which learns tokenization patterns from data.\n\n---\n\n### Applications of Tokenization\n\n1. **Search Engines**: Breaking queries and documents into tokens for efficient matching.\n2. **Machine Translation**: Tokenization ensures consistent splitting of words for alignment between languages.\n3. **Chatbots and Assistants**: Tokenization helps analyze and process user input.\n4. **Text Summarization and Classification**: Tokens serve as input for machine learning models.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"---\n\n### Code Example in Python\n\nHere’s how tokenization works with Python libraries like **NLTK** and **Hugging Face Transformers**:\n\n#### Using NLTK\n```python\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\ntext = \"I love NLP. It’s amazing!\"\ntokens = word_tokenize(text)\nprint(tokens)\n# Output: ['I', 'love', 'NLP', '.', 'It', '’', 's', 'amazing', '!']\n```","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\n\nnltk.download('punkt')\n\ntext = \"I love NLP. It’s amazing!\"\ntokens = word_tokenize(text)\nprint(tokens)\n# Output: ['I', 'love', 'NLP', '.', 'It', '’', 's', 'amazing', '!']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:18:57.685424Z","iopub.execute_input":"2024-12-19T08:18:57.68577Z","iopub.status.idle":"2024-12-19T08:18:59.071315Z","shell.execute_reply.started":"2024-12-19T08:18:57.685743Z","shell.execute_reply":"2024-12-19T08:18:59.070298Z"}},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n['I', 'love', 'NLP', '.', 'It', '’', 's', 'amazing', '!']\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"#### Using Hugging Face Tokenizer\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"I love NLP. It’s amazing!\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# Output: ['i', 'love', 'nlp', '.', 'it', \"'\", 's', 'amazing', '!']\n```","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\ntext = \"I love NLP. It’s amazing!\"\ntokens = tokenizer.tokenize(text)\nprint(tokens)\n# Output: ['i', 'love', 'nlp', '.', 'it', \"'\", 's', 'amazing', '!']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-19T08:19:12.420569Z","iopub.execute_input":"2024-12-19T08:19:12.421109Z","iopub.status.idle":"2024-12-19T08:19:18.559362Z","shell.execute_reply.started":"2024-12-19T08:19:12.421075Z","shell.execute_reply":"2024-12-19T08:19:18.558081Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c09417739c94fb3b359f3515919fd47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92d8501f4d6f4d8ba1e842f18da7df66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffcc5153c2a441f8c39e510f7e3b75c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14ad4e21423140cf9a8fe2a71ba1017b"}},"metadata":{}},{"name":"stdout","text":"['i', 'love', 'nl', '##p', '.', 'it', '’', 's', 'amazing', '!']\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"---\n\n### Challenges in Tokenization\n\n1. **Ambiguity**: Handling different languages and dialects (e.g., Chinese doesn’t use spaces).\n2. **Compound Words**: Splitting or keeping compound words intact (e.g., \"ice-cream\").\n3. **Context Sensitivity**: Properly splitting tokens in context (e.g., \"U.S.A.\" vs \"USA\").\n\n---\n\n### Summary\n\nTokenization is a foundational step in NLP, transforming raw text into manageable pieces for analysis. Advanced techniques like subword tokenization and neural-based methods have significantly improved handling diverse text data in modern AI systems.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}