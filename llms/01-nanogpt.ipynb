{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/shravankumar147/01-nanogpt?scriptVersionId=186347566\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"## Building a GPT\n\nCompanion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T16:16:25.428828Z","iopub.execute_input":"2024-06-30T16:16:25.430016Z","iopub.status.idle":"2024-06-30T16:16:26.833121Z","shell.execute_reply.started":"2024-06-30T16:16:25.429978Z","shell.execute_reply":"2024-06-30T16:16:26.831504Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"--2024-06-30 16:16:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: 'input.txt'\n\ninput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n\n2024-06-30 16:16:26 (24.6 MB/s) - 'input.txt' saved [1115394/1115394]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\nprint(\"length of dataset in characters: \", len(text))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T16:16:26.835735Z","iopub.execute_input":"2024-06-30T16:16:26.836171Z","iopub.status.idle":"2024-06-30T16:16:26.845237Z","shell.execute_reply.started":"2024-06-30T16:16:26.836133Z","shell.execute_reply":"2024-06-30T16:16:26.843927Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"length of dataset in characters:  1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's look at the first 1000 characters\nprint(text[:1000])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T16:16:26.846852Z","iopub.execute_input":"2024-06-30T16:16:26.8473Z","iopub.status.idle":"2024-06-30T16:16:26.85849Z","shell.execute_reply.started":"2024-06-30T16:16:26.847268Z","shell.execute_reply":"2024-06-30T16:16:26.857078Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"First Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-30T16:16:26.861161Z","iopub.execute_input":"2024-06-30T16:16:26.86155Z","iopub.status.idle":"2024-06-30T16:16:26.901129Z","shell.execute_reply.started":"2024-06-30T16:16:26.861519Z","shell.execute_reply":"2024-06-30T16:16:26.899797Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n","output_type":"stream"}]},{"cell_type":"code","source":"# create a mapping from characters to integers\n\nstoi = {ch:i for i,ch in enumerate(chars)}\nitos = {i:ch for i,ch in enumerate(chars)}\n\nencode = lambda s: [stoi[c] for c in s]\ndecode = lambda l: \"\".join([itos[i] for i in l])","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:26.902572Z","iopub.execute_input":"2024-06-30T16:16:26.903054Z","iopub.status.idle":"2024-06-30T16:16:26.914272Z","shell.execute_reply.started":"2024-06-30T16:16:26.90302Z","shell.execute_reply":"2024-06-30T16:16:26.912935Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"print(encode(\"hello\"))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:26.916033Z","iopub.execute_input":"2024-06-30T16:16:26.916492Z","iopub.status.idle":"2024-06-30T16:16:26.927653Z","shell.execute_reply.started":"2024-06-30T16:16:26.916451Z","shell.execute_reply":"2024-06-30T16:16:26.926344Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[46, 43, 50, 50, 53]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(encode(\"hello\")))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:26.930813Z","iopub.execute_input":"2024-06-30T16:16:26.932064Z","iopub.status.idle":"2024-06-30T16:16:26.941649Z","shell.execute_reply.started":"2024-06-30T16:16:26.932022Z","shell.execute_reply":"2024-06-30T16:16:26.94036Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"hello\n","output_type":"stream"}]},{"cell_type":"code","source":"# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch\n\nprint(f\"No. of chars: {len(text)}\")\nprint(f\"No. of encoded integers: {len(encode(text))}\") # => 1115394; \nassert len(text)==len(encode(text)) #this confirms our ecoder is working fine\n","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:27.279626Z","iopub.execute_input":"2024-06-30T16:16:27.28008Z","iopub.status.idle":"2024-06-30T16:16:27.495239Z","shell.execute_reply.started":"2024-06-30T16:16:27.280045Z","shell.execute_reply":"2024-06-30T16:16:27.493833Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"No. of chars: 1115394\nNo. of encoded integers: 1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"encoded_text = encode(text)\ndata = torch.tensor(encoded_text, dtype=torch.long)\n\nprint(data.shape, data.dtype)\n\nprint(data[:1000])","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:27.707305Z","iopub.execute_input":"2024-06-30T16:16:27.707738Z","iopub.status.idle":"2024-06-30T16:16:27.999822Z","shell.execute_reply.started":"2024-06-30T16:16:27.707705Z","shell.execute_reply":"2024-06-30T16:16:27.998595Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"torch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's now split up the data into train and validation sets\n# first 90% will be train, rest val\nprint(f\"Data Size: {len(data)}\")\nn = int(len(data)*0.9)\n\nprint(f\"90%: {len(data[:n])}| 10%: {len(data[n:])}\")\n\nprint(f\"90%: {len(data[:n])} + 10%: {len(data[n:])} =  {len(data[:n]) + len(data[n:])}\")\nassert len(data[:n]) + len(data[n:]) == len(data)\n\n\ntrain_data = data[:n]\nval_data = data[n:]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:28.005525Z","iopub.execute_input":"2024-06-30T16:16:28.00595Z","iopub.status.idle":"2024-06-30T16:16:28.014985Z","shell.execute_reply.started":"2024-06-30T16:16:28.005901Z","shell.execute_reply":"2024-06-30T16:16:28.013625Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Data Size: 1115394\n90%: 1003854| 10%: 111540\n90%: 1003854 + 10%: 111540 =  1115394\n","output_type":"stream"}]},{"cell_type":"code","source":"block_size = 8 # also known as context length, that a transformer see\n\ntrain_data[:block_size+1]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:28.321064Z","iopub.execute_input":"2024-06-30T16:16:28.321502Z","iopub.status.idle":"2024-06-30T16:16:28.331206Z","shell.execute_reply.started":"2024-06-30T16:16:28.321468Z","shell.execute_reply":"2024-06-30T16:16:28.32986Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"},"metadata":{}}]},{"cell_type":"code","source":"x = train_data[:block_size]\ny = train_data[1:block_size+1]\n\nprint(f\"input context-> target\")\nprint(\"=\"*50)\n\nfor t in range(block_size):    \n    print(f\"{x[:t+1]} -> {y[t]}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:28.635262Z","iopub.execute_input":"2024-06-30T16:16:28.635751Z","iopub.status.idle":"2024-06-30T16:16:28.64768Z","shell.execute_reply.started":"2024-06-30T16:16:28.635705Z","shell.execute_reply":"2024-06-30T16:16:28.646222Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"input context-> target\n==================================================\ntensor([18]) -> 47\ntensor([18, 47]) -> 56\ntensor([18, 47, 56]) -> 57\ntensor([18, 47, 56, 57]) -> 58\ntensor([18, 47, 56, 57, 58]) -> 1\ntensor([18, 47, 56, 57, 58,  1]) -> 15\ntensor([18, 47, 56, 57, 58,  1, 15]) -> 47\ntensor([18, 47, 56, 57, 58,  1, 15, 47]) -> 58\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    data = train_data if split=='train' else val_data\n    \n    # This line generates batch_size random indices (ix) within the range from 0 to len(data) - block_size. \n    # The subtraction by block_size ensures there’s enough room to create sequences of length block_size.\n    ix = torch.randint(low=0, high=len(data)-block_size, size=(batch_size,)) # we get batch_size random integers within the range\n    \n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    \n    return x,y","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:29.625272Z","iopub.execute_input":"2024-06-30T16:16:29.625679Z","iopub.status.idle":"2024-06-30T16:16:29.636377Z","shell.execute_reply.started":"2024-06-30T16:16:29.62565Z","shell.execute_reply":"2024-06-30T16:16:29.634982Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"xb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:30.404818Z","iopub.execute_input":"2024-06-30T16:16:30.405289Z","iopub.status.idle":"2024-06-30T16:16:30.417988Z","shell.execute_reply.started":"2024-06-30T16:16:30.405255Z","shell.execute_reply":"2024-06-30T16:16:30.416421Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"inputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\n","output_type":"stream"}]},{"cell_type":"code","source":"for b in range(batch_size):\n    for t in range(block_size):\n        context = xb[b,:t+1]\n        target = yb[b, t]\n        \n        print(f\"when the input is: {context.tolist()}, the target is: {target}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:30.729128Z","iopub.execute_input":"2024-06-30T16:16:30.729557Z","iopub.status.idle":"2024-06-30T16:16:30.739592Z","shell.execute_reply.started":"2024-06-30T16:16:30.729525Z","shell.execute_reply":"2024-06-30T16:16:30.738301Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"when the input is: [24], the target is: 43\nwhen the input is: [24, 43], the target is: 58\nwhen the input is: [24, 43, 58], the target is: 5\nwhen the input is: [24, 43, 58, 5], the target is: 57\nwhen the input is: [24, 43, 58, 5, 57], the target is: 1\nwhen the input is: [24, 43, 58, 5, 57, 1], the target is: 46\nwhen the input is: [24, 43, 58, 5, 57, 1, 46], the target is: 43\nwhen the input is: [24, 43, 58, 5, 57, 1, 46, 43], the target is: 39\nwhen the input is: [44], the target is: 53\nwhen the input is: [44, 53], the target is: 56\nwhen the input is: [44, 53, 56], the target is: 1\nwhen the input is: [44, 53, 56, 1], the target is: 58\nwhen the input is: [44, 53, 56, 1, 58], the target is: 46\nwhen the input is: [44, 53, 56, 1, 58, 46], the target is: 39\nwhen the input is: [44, 53, 56, 1, 58, 46, 39], the target is: 58\nwhen the input is: [44, 53, 56, 1, 58, 46, 39, 58], the target is: 1\nwhen the input is: [52], the target is: 58\nwhen the input is: [52, 58], the target is: 1\nwhen the input is: [52, 58, 1], the target is: 58\nwhen the input is: [52, 58, 1, 58], the target is: 46\nwhen the input is: [52, 58, 1, 58, 46], the target is: 39\nwhen the input is: [52, 58, 1, 58, 46, 39], the target is: 58\nwhen the input is: [52, 58, 1, 58, 46, 39, 58], the target is: 1\nwhen the input is: [52, 58, 1, 58, 46, 39, 58, 1], the target is: 46\nwhen the input is: [25], the target is: 17\nwhen the input is: [25, 17], the target is: 27\nwhen the input is: [25, 17, 27], the target is: 10\nwhen the input is: [25, 17, 27, 10], the target is: 0\nwhen the input is: [25, 17, 27, 10, 0], the target is: 21\nwhen the input is: [25, 17, 27, 10, 0, 21], the target is: 1\nwhen the input is: [25, 17, 27, 10, 0, 21, 1], the target is: 54\nwhen the input is: [25, 17, 27, 10, 0, 21, 1, 54], the target is: 39\n","output_type":"stream"}]},{"cell_type":"code","source":"print(xb) # our input to the transformer","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:31.126077Z","iopub.execute_input":"2024-06-30T16:16:31.127291Z","iopub.status.idle":"2024-06-30T16:16:31.134716Z","shell.execute_reply.started":"2024-06-30T16:16:31.127251Z","shell.execute_reply":"2024-06-30T16:16:31.133251Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:31.462371Z","iopub.execute_input":"2024-06-30T16:16:31.462825Z","iopub.status.idle":"2024-06-30T16:16:31.474752Z","shell.execute_reply.started":"2024-06-30T16:16:31.462789Z","shell.execute_reply":"2024-06-30T16:16:31.473415Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f9580e557f0>"},"metadata":{}}]},{"cell_type":"code","source":"emb = nn.Embedding(vocab_size, vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:31.832524Z","iopub.execute_input":"2024-06-30T16:16:31.832988Z","iopub.status.idle":"2024-06-30T16:16:31.840439Z","shell.execute_reply.started":"2024-06-30T16:16:31.832952Z","shell.execute_reply":"2024-06-30T16:16:31.839052Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"Here we test the Embedding layer","metadata":{}},{"cell_type":"markdown","source":"It accepts a tensor as input","metadata":{}},{"cell_type":"code","source":"print(emb(torch.tensor(0))) # for a 1-d tensor it has generated below output\nprint(emb(torch.tensor(0)).shape) # the shape is 65, which are embeddings from the embedding lookup table","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:33.090843Z","iopub.execute_input":"2024-06-30T16:16:33.091306Z","iopub.status.idle":"2024-06-30T16:16:33.105234Z","shell.execute_reply.started":"2024-06-30T16:16:33.091271Z","shell.execute_reply":"2024-06-30T16:16:33.103815Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"tensor([ 0.1808, -0.0700, -0.3596, -0.9152,  0.6258,  0.0255,  0.9545,  0.0643,\n         0.3612,  1.1679, -1.3499, -0.5102,  0.2360, -0.2398, -0.9211,  1.5433,\n         1.3488, -0.1396,  0.2858,  0.9651, -2.0371,  0.4931,  1.4870,  0.5910,\n         0.1260, -1.5627, -1.1601, -0.3348,  0.4478, -0.8016,  1.5236,  2.5086,\n        -0.6631, -0.2513,  1.0101,  0.1215,  0.1584,  1.1340, -1.1539, -0.2984,\n        -0.5075, -0.9239,  0.5467, -1.4948, -1.2057,  0.5718, -0.5974, -0.6937,\n         1.6455, -0.8030,  1.3514, -0.2759, -1.5108,  2.1048,  2.7630, -1.7465,\n         1.4516, -1.5103,  0.8212, -0.2115,  0.7789,  1.5333,  1.6097, -0.4032,\n        -0.8345], grad_fn=<EmbeddingBackward0>)\ntorch.Size([65])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can pass a batch of input to the embedding layer, so let's pass our xb batch","metadata":{}},{"cell_type":"code","source":"xb.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:33.787692Z","iopub.execute_input":"2024-06-30T16:16:33.788136Z","iopub.status.idle":"2024-06-30T16:16:33.796403Z","shell.execute_reply.started":"2024-06-30T16:16:33.788103Z","shell.execute_reply":"2024-06-30T16:16:33.794962Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8])"},"metadata":{}}]},{"cell_type":"code","source":"logits = emb(xb)\nB,T,C = logits.shape\nprint(B,T,C)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:34.116342Z","iopub.execute_input":"2024-06-30T16:16:34.116781Z","iopub.status.idle":"2024-06-30T16:16:34.123901Z","shell.execute_reply.started":"2024-06-30T16:16:34.116746Z","shell.execute_reply":"2024-06-30T16:16:34.122553Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"4 8 65\n","output_type":"stream"}]},{"cell_type":"markdown","source":"4 is the batch size\n\n8 is the block size (time-dimention)\n\n65 is the output embedding (channel -dimention)","metadata":{}},{"cell_type":"markdown","source":"Let's do some reshaping - this will come handy when we compute cross-entropy loss for our Bigram model","metadata":{}},{"cell_type":"code","source":"logits.view(B*T, C).shape # using view approach","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:35.137416Z","iopub.execute_input":"2024-06-30T16:16:35.137817Z","iopub.status.idle":"2024-06-30T16:16:35.146062Z","shell.execute_reply.started":"2024-06-30T16:16:35.137788Z","shell.execute_reply":"2024-06-30T16:16:35.144794Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 65])"},"metadata":{}}]},{"cell_type":"code","source":"logits.flatten(start_dim=0, end_dim=1).shape # using flatten approach","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:35.480491Z","iopub.execute_input":"2024-06-30T16:16:35.48088Z","iopub.status.idle":"2024-06-30T16:16:35.489942Z","shell.execute_reply.started":"2024-06-30T16:16:35.480851Z","shell.execute_reply":"2024-06-30T16:16:35.488641Z"},"trusted":true},"execution_count":37,"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"torch.Size([32, 65])"},"metadata":{}}]},{"cell_type":"markdown","source":"We can use any method, so I am chosing view, because I can use my pre computed B,T, and C there","metadata":{}},{"cell_type":"code","source":"class BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n        \n    def forward(self, idx, targets=None):\n        logits = self.token_embedding_table(idx)\n        if targets is None:\n            loss = None\n        else:\n            \n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            \n            loss = F.cross_entropy(logits, targets)\n            \n        return logits, loss\n        \n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            logits, loss = self(idx) # Logits has shape: (B, T, C)\n            \n            # Let's take the last time step from the logits/predictions\n            logits = logits[:,-1,:] #(B, C)\n            \n            # apply softmax to get the probabilities of these predictions\n            # over the Channels dimensions. \n            probs = F.softmax(logits, dim=-1) #(B, C)\n            \n            # torch.multinomial returns a tensor where each row contains num_samples indices \n            # sampled from the multinomial probability distribution located in the corresponding \n            # row of tensor input.\n            \n            idx_next = torch.multinomial(probs, num_samples=1)\n            \n            # add the next probable prediction to the current idx batch.\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n            \n        return idx   \n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:36.06987Z","iopub.execute_input":"2024-06-30T16:16:36.070346Z","iopub.status.idle":"2024-06-30T16:16:36.082765Z","shell.execute_reply.started":"2024-06-30T16:16:36.070313Z","shell.execute_reply":"2024-06-30T16:16:36.081205Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"m = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:36.348043Z","iopub.execute_input":"2024-06-30T16:16:36.348439Z","iopub.status.idle":"2024-06-30T16:16:36.364959Z","shell.execute_reply.started":"2024-06-30T16:16:36.348403Z","shell.execute_reply":"2024-06-30T16:16:36.36353Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"torch.Size([32, 65])\ntensor(4.6630, grad_fn=<NllLossBackward0>)\n","output_type":"stream"}]},{"cell_type":"code","source":"print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:36.666939Z","iopub.execute_input":"2024-06-30T16:16:36.667376Z","iopub.status.idle":"2024-06-30T16:16:36.701673Z","shell.execute_reply.started":"2024-06-30T16:16:36.667345Z","shell.execute_reply":"2024-06-30T16:16:36.700533Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\nhbH\n\n:CLP.A!fq'3ggt!O!T?X!!SA?W&TrpvYybSE3w&S BXUhmiKYyTmWMPhhmnHKj!!btgnwNNULuEzRuYyiWEQxPX!$3C'MBj\n","output_type":"stream"}]},{"cell_type":"markdown","source":"That's something so random, that means we need to learn to make good predictions","metadata":{}},{"cell_type":"code","source":"# create a pytorch optiizer \noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:37.392844Z","iopub.execute_input":"2024-06-30T16:16:37.393243Z","iopub.status.idle":"2024-06-30T16:16:39.142572Z","shell.execute_reply.started":"2024-06-30T16:16:37.393215Z","shell.execute_reply":"2024-06-30T16:16:39.141231Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# training loop\n# get batches -> compute loss -> get gradients from the computed loss -> updated model params \n\nbatch_size = 32\nnum_epochs = 10000\n\nfor steps in range(num_epochs):\n    \n    #sample the batches from the data\n    xb, yb = get_batch('train')\n    \n    # evaluate the loss\n    logits, loss = m(xb,yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    \n    if steps%1000==0:\n        print(f\"step: {steps} - {loss.item()}\")\n        \nprint(f\"\\nfinal loss at end of epoch {steps}: {loss.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:16:39.144728Z","iopub.execute_input":"2024-06-30T16:16:39.145242Z","iopub.status.idle":"2024-06-30T16:17:00.685152Z","shell.execute_reply.started":"2024-06-30T16:16:39.14521Z","shell.execute_reply":"2024-06-30T16:17:00.683698Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"step: 0 - 4.641923427581787\nstep: 1000 - 3.615529775619507\nstep: 2000 - 3.0686445236206055\nstep: 3000 - 2.7126073837280273\nstep: 4000 - 2.6058950424194336\nstep: 5000 - 2.6941914558410645\nstep: 6000 - 2.480062246322632\nstep: 7000 - 2.421450614929199\nstep: 8000 - 2.602724075317383\nstep: 9000 - 2.419046640396118\n\nfinal loss at end of epoch 9999: 2.451293468475342\n","output_type":"stream"}]},{"cell_type":"code","source":"idx = torch.zeros((1, 1), dtype=torch.long)\nidx_new = m.generate(idx, max_new_tokens=500)\n\nprint(decode(idx_new[0].tolist()))","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:17:00.687027Z","iopub.execute_input":"2024-06-30T16:17:00.687403Z","iopub.status.idle":"2024-06-30T16:17:00.768594Z","shell.execute_reply.started":"2024-06-30T16:17:00.687371Z","shell.execute_reply":"2024-06-30T16:17:00.767388Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"\nCI\nTEE:\nNClucor che t thendouilftheesco'imin?\nDUESp;\nDupll nn owe hisaloriclfer eened de ms, sth:\nMI dintearow, att t isin aren gs fran bag ndais theer so,\n\nIVI mounce s pr higay'stindinthe,\n\nWhy w'surnurn myof bby fedise?\nThertle:\nLOLO, e te.\nFzirJond f:\nS: winan w, KI ardathart.\n\nD m arere toor!\nArgo sh meas hirend\n\n\nAs anicrirtopowh ovend sinouce doreaklanoreseranmye, bithorer glre?\nARS: t i'CLown athe is! hinondar ate t y poames.\nThathim IR.\nOTryowifromy te pehothitowild. hoveaswecestharchel\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## The mathematical trick in self-attention","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:17:15.062238Z","iopub.execute_input":"2024-06-30T16:17:15.062677Z","iopub.status.idle":"2024-06-30T16:17:15.075159Z","shell.execute_reply.started":"2024-06-30T16:17:15.062642Z","shell.execute_reply":"2024-06-30T16:17:15.073844Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"a=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n","output_type":"stream"}]},{"cell_type":"code","source":"# consider the following toy example:\n\ntorch.manual_seed(1338)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:29:33.398593Z","iopub.execute_input":"2024-06-30T16:29:33.399045Z","iopub.status.idle":"2024-06-30T16:29:33.409614Z","shell.execute_reply.started":"2024-06-30T16:29:33.39901Z","shell.execute_reply":"2024-06-30T16:29:33.408305Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"torch.Size([4, 8, 2])"},"metadata":{}}]},{"cell_type":"code","source":"# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:29:35.526613Z","iopub.execute_input":"2024-06-30T16:29:35.527063Z","iopub.status.idle":"2024-06-30T16:29:35.535635Z","shell.execute_reply.started":"2024-06-30T16:29:35.527029Z","shell.execute_reply":"2024-06-30T16:29:35.534462Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"print(xprev.shape)\ntorch.mean(xprev, 0)\n# xbow[0,2]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:33:03.990961Z","iopub.execute_input":"2024-06-30T16:33:03.991615Z","iopub.status.idle":"2024-06-30T16:33:04.003144Z","shell.execute_reply.started":"2024-06-30T16:33:03.991574Z","shell.execute_reply":"2024-06-30T16:33:04.00133Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"torch.Size([8, 2])\n","output_type":"stream"},{"execution_count":70,"output_type":"execute_result","data":{"text/plain":"tensor([-0.0373, -0.6127])"},"metadata":{}}]},{"cell_type":"code","source":"# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nprint(wei.shape, x.shape)\n\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)  i.e (T,T) X (T,C) --> (T,C) with batch B\n# xbow2\ntorch.allclose(xbow, xbow2)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:34:18.081748Z","iopub.execute_input":"2024-06-30T16:34:18.082324Z","iopub.status.idle":"2024-06-30T16:34:18.094667Z","shell.execute_reply.started":"2024-06-30T16:34:18.08228Z","shell.execute_reply":"2024-06-30T16:34:18.093472Z"},"trusted":true},"execution_count":73,"outputs":[{"name":"stdout","text":"torch.Size([8, 8]) torch.Size([4, 8, 2])\n","output_type":"stream"},{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# # Step 1: Add a new dimension at position 0\n# wei = wei.unsqueeze(0)  # Shape becomes [1, 8, 8]\n\n# # Step 2: Repeat the tensor along the new dimension\n# wei = wei.repeat(4, 1, 1)  # Shape becomes [4, 8, 8]\n\n# print(wei.shape)  # Output: torch.Size([4, 8, 8])","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:37:31.69327Z","iopub.execute_input":"2024-06-30T16:37:31.693776Z","iopub.status.idle":"2024-06-30T16:37:31.702302Z","shell.execute_reply.started":"2024-06-30T16:37:31.693742Z","shell.execute_reply":"2024-06-30T16:37:31.7008Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 8])\n","output_type":"stream"}]},{"cell_type":"code","source":"# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:40:21.515099Z","iopub.execute_input":"2024-06-30T16:40:21.51551Z","iopub.status.idle":"2024-06-30T16:40:21.526128Z","shell.execute_reply.started":"2024-06-30T16:40:21.51548Z","shell.execute_reply":"2024-06-30T16:40:21.52481Z"},"trusted":true},"execution_count":78,"outputs":[{"execution_count":78,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# version 4: self- attention\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ntorch.manual_seed(1337)\n\nB, T, C = 4, 8, 32\nhead_size = 16\n\nx = torch.randn(B,T,C)\n\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False) \nvalue = nn.Linear(C, head_size, bias=False) \n\n\nk = key(x)  # (B,T,C)(C,head_size) -> (B,T,head_size)\nq = query(x) # (B,T,C)(C,head_size) -> (B,T,head_size)\n\n\n\nwei = q@k.transpose(-2,-1)  # (B,T,head_size) @ (B,head_size,T) --> (B, T, T)\n\n# Decoder , we don't want the communication happen from the future tokens in the attention mechanism\ntril = torch.tril(torch.ones(T,T))\nwei = wei.masked_fill(tril==0, float(\"-inf\")) # (B, T, T)\n\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\n\nout = wei@v  # (B,T,T) @ (B,T,C)  --> (B,T,C)\n\nprint(out.shape)\n \n","metadata":{"execution":{"iopub.status.busy":"2024-07-01T09:02:08.076698Z","iopub.execute_input":"2024-07-01T09:02:08.077135Z","iopub.status.idle":"2024-07-01T09:02:08.093133Z","shell.execute_reply.started":"2024-07-01T09:02:08.077094Z","shell.execute_reply":"2024-07-01T09:02:08.091606Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"torch.Size([4, 8, 16])\n","output_type":"stream"}]},{"cell_type":"code","source":"wei[0]","metadata":{"execution":{"iopub.status.busy":"2024-07-01T09:02:09.131244Z","iopub.execute_input":"2024-07-01T09:02:09.131637Z","iopub.status.idle":"2024-07-01T09:02:09.140584Z","shell.execute_reply.started":"2024-07-01T09:02:09.13161Z","shell.execute_reply":"2024-07-01T09:02:09.139562Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)"},"metadata":{}}]},{"cell_type":"code","source":"out[0].shape","metadata":{"execution":{"iopub.status.busy":"2024-07-01T09:02:11.143624Z","iopub.execute_input":"2024-07-01T09:02:11.144051Z","iopub.status.idle":"2024-07-01T09:02:11.151336Z","shell.execute_reply.started":"2024-07-01T09:02:11.144017Z","shell.execute_reply":"2024-07-01T09:02:11.150231Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 16])"},"metadata":{}}]},{"cell_type":"markdown","source":"Notes:\n- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}